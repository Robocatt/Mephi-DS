### Книги по рекомендации Бидзили
Introduction to Reliable and Secure Distributed Programming - теория
Designing Data-Intensive Applications The Big Ideas Behind Reliable, Scalable and Maintainable Systems - практика
[Плейлист](https://www.youtube.com/playlist?list=PL4_hYwCyhAvaYKF6HkyCximCvlExxxnrC) Романа Липовского для мфти
## Лекции в МИФИ
### 04.09.24 Лекция 1
Иерархичность серверов => Главные меняются. Clos архитектура.
Система хостов полносвязана, имеет локальные диски. Общих данных нет, только обмен сообщениями.
Клиент как именно часть распред системы.
- Отказоустойчивость
- Не хватает одного хоста (вычисления/память)

Задача "консенсуса" как самая сложная в распределенных системах.

Задача: (похожа на  nosql )
key-val storage 
set(k, v); get(k) -> v.
Подразумевает асинхронность, но ответ даже на set необходим обязательно.
Лучше не отвечать, пока гарантированно не записал, у себя и желательно в остальных хостах.

Игнорим ошибки памяти(византийские гарантии),cpu, шанс что диск записал не то (check sum).
Host: 
- crash free;
- crash stop - сервер не реснется; 
- crash recovery может восстанавливаться и умирать постоянно. или полностью умереть как в crash stop.

Сеть - пакеты:  пусть нет corrupt, но может быть drop, а также duplicate. 

Fair loss  - рано или поздно сообщение будет принято

Perfect link - идеальная модель в которой все доходит 1 раз. 
Но даже в ней с умирающими хостами приколы.

Fairloss + tcp/ip = perfect link в достаточных для практики  пределах
Работаем с perfect link но не смотря на это помним про проблемы 

1.  Отказоустойчивость: не смотря на смерть хостов данные будут сохранены  
2. Масштабируемость (вплоть до линейной)
3. Консистентность

~40 шт хостов - стойка. Обмен данными к хостам через общий switch.
Тогда важно писать в хосты на разных стойках.
Иерархия доменов отказа.

round trip time 1-3 ms в data center 

Два вида свойств/гарантий в распределенной системе:
1. Safety Система ведет себя или корректно или нет.
2. Liveness . Три режима ответа сети. О времени ответа сети,  том, когда будет полезный ответ. Асинхронная, синхронная, частичная. Частичную рассматриваем. 

Время и часы.
Дрейф часов. Кварцевые ~1.7 sec/day. Атомные. Можно сказать не дрейфуют.
Но синхронизация часов тоже требует время.
Физические часы:
wall clock - аппаратные. Синхронизируются, есть високосные секунды.
monotonic clock - Локально. С момента старта. Никогда не откатываются.
Логические часы.

не можем синхронизировать больше чем eps.

### 07.09.24 Семинар 1
Сем. Ссылки с него 
Некий [userver](https://userver.tech/)
Описание про [thread](https://wiki.osdev.org/Thread)
[Примеры](https://github.com/chriskohlhoff/asio/tree/master/asio/src/examples) асинхронности, синхронности сервера



### 11.09.24 Лекция 2
K-v storage.
Regular Register.
Write (val)
Read->val

Single write. 
3  хоста всего. Можно ли писать на 3 хоста? На 1 нельзя, так как он сам может умереть. На три нельзя, потому что хосты умирают и если будем писать на 3 при смерти хоста запись не будет выполнена. Пусть пишем хотя бы на 2 хоста всегда. 

Надо выбрать все три  и дождаться ответ от двух.

Читать с 1го хоста? Нет, может умереть.  Или может сохранять старое значение. 
Читать с 3-х хостов, нет так как если при такой системе умрет один алгоритм никогда не завершится.
Читаем с 2-х хостов. 

Соответственно пишем/читаем на 2 хоста.  Тогда если читаем, то обязательно пересечемся хотя бы с 1-м из тех хостов на который писали. Кворум. Читаем кворум, пишем кворум. 
Как определить где новые данные? **Timestamp**.Их генерит Single writer. Именно single, что бы можно было просто ++ на счетчик и ничего не синхронизировать. Логическое время.
Что-то типа requestId не работает. Так как они не упорядочены. Упорядочевать id в распределенных системах - задача сложная. 


В целом думают вначале так, что у нас есть лидер, потом решают как от этого избавиться т.е. как выбирать лидера.
| w1  |   |                         w2          |  
   |     r2   |
          |    r1            |
Такое возможно. Так как для  read2 может быть кворум:
 |         /    |  
 |2    /1/    |  
 |    |  | | |  | |   
|        / 1  / |  
     /

Write majority.
N хостов. 
2N -> N+1 пишем .  Fault tollerance  N - 1
2N +1 ->N+1 пишем. Fault tollerance N
Fault tollerance  - максимальное количество отказавших хостов. 

Кворум- любая система множеств в которой любые 2 множества пересекаются. 

Ассиметричный кворум - пишем на 4, читаем с 2-х. Типо чтение - чтение могут не пересечься. Используется когда очень много чтений.

Четное число хостов не уважают и считают что так не делают.
Так как кворум одинаковый, а Ft меньше.

Модели консистентности. 
Самая "слабая":
Eventual consistency  - когда нибудь все будет отлично и система придет к консистентному состоянию.

Самое "сильное" свойство.
Linearizability
Linearization point.
Операция произошла в какой то **определенный** момент времени. До него операции не было, после точки она есть.

Sequential consistency.
Разложение истории на временную ось без пересечений. 


Atomic register. 
Read Impose 
|read from quorum| impose |
impose = write  on quorum



### 14.09.24 Семинар 2
Процесс - единица исполнения, обладающая ресурсами(памятью)
Поток - -//- не имеет своей памяти 
На стороне OS и управляет OS через sheduller с вытесняющей многозадачностью.
FIber - кооперативная многозадачность. Работает пока сам не остановит себя.
Subroutine - просто функция  в ЯП
Coroutine - функция, что может быть остановлена и потом быть восстановлена с того момента, на котором была остановлена.

Stackfull / Stackless 

Callback генерятся сами внутри co_await 
В stackfull нет этих слов, там будет прям объект coroutine и его pure методы.

Примитивы синхронизации для fiber будут немного отличаться, ведь надо стопить определенный fiber, а не весь thread.

Courutine - при использовании co_wait, сама вставляет callback.

[Фреймворки](https://www.techempower.com/benchmarks/#hw=ph&test=fortune&section=data-r22) с fiber.

Userver
Почиать [документацию](https://userver.tech/)


### 18.09.24 Лекция 3
1 Writer -> multiple writers. 
Было у каждого write некоторый строго увеличивающийся timestamp. Порядок строился на них.
R  = R + W
W = R 
Собрали кворум на R из этого read получили timestamp потом делаем W (timestamp + 1)

Пришло два w по одному timestamp. Надо линеаризовать => упорядочить, но не важно как. Тогда добавим к timestamp у W  какой-то глоабльный уникальный ключ - GUID. И все, посортим по нему, ведь порядок должен быть любой. 

Доказательство линеаризуемости. 
    01
R + w         O2
          R + w
Кворум на O1 W и  O2 R пересекутся, получим из них timestamp. 
timestamp O1<=timestamp O2


Операция CAS. Compare and swap
```
CAS(k, v, v*)
	if (stor[k] == v){
	stor[k]=v*;
}
```

На какждом хосте должен быть один и тот же порядок что бы CAS работал корректно.
(Тогда CAS должен быть блокирующим для машины?)

Replicated State Machine.
	A replicated state machine is **a deterministic state machine where multiple machines have the same state and run the same**.
({stat}, {transition}). Transition - какой-то переход  внутри хоста по автомату.  
Хотим автомат разложить на все реплики в начальном состоянии? 
Хотим что б ы на каждом хосте операции прмиенились в одном порядке, что бы в конце концов все хосты были в одном и том же состояниию.
?????

Atomic Broadcast
A-Broadcast(m) <- отправить всем сообщение 
A-Deliver(m) <- обработчик
Свойства ^7c1e2a
- Validity, если не сбойный узел сделал  A-Broadcast(m) то он позовет A-delivery.
	Узел сбойный тогда, когда он умирает и не взаимодействует больше с другими узлами. Все остальные узлы не сбойные. 
- Agreement , если на не сбойном хосте позвался delivery(m) , то на всех остальных он тоже позавется. 
Получаем гарантию что все сообщения будут получены несбойными хостами. 
- **Total order**, Для двух хостов посмотрим их историю в любой момент времени. Тогда один из них является префиксом другого или наоборот.  

RSM с помощью Atomic Broadcast.
Broadcast оставляем.
Deliver - переключить состояние. 
При запросе: Broadacst, ждем deliver, при получении отправляем ответ клиенту.

Reg < Reg + CAS < RSM < AB < Consensus


Linearizability AB

То есть по total order другого порядка нигде быть не может, а значит и линеаризуемо.

Если узел координатор(к которому в начале пришел клиент) умер до отправки ответа клиенту, а такое может быть. То, это уже должен фиксить клиент. То есть если он не получил ответ, то он должен еще раз сходить в систему и сделать check или retry.// Exactly once нигде нет. 

Mutex = RSM 
Node &harr; Node

RSM умеет переживать перезапуски машин!

Propose(v) -> v\* 
v\* вернется на все хосты, причем $v* \in {v1,v2,v3}$

Validity - safety (на конечном промежутке)
Agreement - safety (на конечном промежутке)
Termination - Liveness ( на бесконечности)

AB:
Validity - liveness
Agreement - liveness
Total order - safety  


AB -> Consensus
Ab(m1), delivery вернет то, что типо должен вернуть propose
Ab(m2)
Ab(m3)

Consensus -> AB
Reliable Broadcast

Если сообщение пришло на хост первый раз то еще раз отсылаем всем и потом отправляем обработчик не дожидаясь ответа. 
AB: 
R-B(m) + R-D(m): Buf += m

Background: постоянно крутится процесс. 
Несколько раундов консенсуса. A = отправленные сообщения 
while buffer - A not empty{ 
propose(всех сообщений из buffer - A), вернет R\*
Ad(r\*)
}

### Лекция 4

~Кривое доказательство что cas не работает без state machine.

Алгоритм  работы AB. Смотри [[Распределенные системы#^fd9023]]

Задача бинарного консенсуса. $v* \in {0,1}$

Мы не отличаем partition от отказа узлов. 
При разделении посчитаем, что меньшая часть не будет выполняться. 
Тогда любой алгоритм консенсуса переживает $\frac{N - 1}{2}$ отказов.

[Теорема FLP.](https://neerc.ifmo.ru/wiki/index.php?title=%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%A4%D0%B8%D1%88%D0%B5%D1%80%D0%B0-%D0%9B%D0%B8%D0%BD%D1%87%D0%B0-%D0%9F%D0%B0%D1%82%D0%B5%D1%80%D1%81%D0%BE%D0%BD%D0%B0_(FLP))
Опр. Для любого алгоритма консенсуа который переживает хотя бы 1 отказ существует такое исполнение что алгоритм не завершится. 

Говорим что никогда не нарушаем Validity и Agreement. Значит можем нарушать Termination.
Доказательство. 

Конфигурация - состояние всех узлов + все сообщения в сети. Типо Snapshot. 
Есть направленные переходы между узлами: Доставили сообщение m.
Получили так граф конфигураций.

Исполнение - путь в графе = порядок доставки сообщений. 

Графом конфигурации описывается любой алгоритм консенсуа. Конкретное исполнение - конкретный путь. 

Бесконечное исполнение - бесконечный путь ИЛИ цикл.

Доказательство итеративное.

Лемма 1. ~Существует начальное состояние нам нужное.  = начальное бивалентное состояние. 
Для бинарного консенсуса. Каждая нода позвола propose.  Получается  $2^{n}$ начальных конфигураций.
Валентность конфигурации - то что вернет консенсус.
Конфигурация унивалентная есть мы знаем с каким значением завершится алгоритм. 
Конфигурация бивалентная если не знаем.

- 0 валентная

$$
R_{0} = \begin{pmatrix}
0 \\
\dots \\
0\end{pmatrix}
$$ 

- 1 валентная

$$
R_{n} = \begin{pmatrix}
1 \\
\dots \\
1\end{pmatrix}
$$ 

$$\Rightarrow \exists k : R_{k} \text{0 валетная, } R_{k + 1} \text{1 валетная,} $$
Они отличаются только значением k+1 хоста. Так как алгоритм переживает паделние 1 узла, то этот k+1 узел можно выкинуть. А значит оба этих узла бивалентные. 


Лемма 2. ~Всегда существует переход, который приводит в не завершение. = для любой бивалентной конфигурации можно найти следующую за ней бивалентную.
$$\forall (p,m), \forall c \text{ бивалентное }\exists \text{ путь } c \to c' \text{ что (p,m) , будет доставлен последним в этом пути и  }c' \text{ бивалентное }$$

Причем в этом пути не должно быть более 1 хоста умерло. 
S - множество конфигураций где (p, m) не доставлено
S' доставлено из S (p, m) последним.
А за границей этого контура что-то что после s'.
![S Sets](img/20240925_094206.jpg)

Лемма 2.1. Хотим доказать что в S' есть бивалентное состояние.
Докажем от противного. Пусть там только унивалентные. 
Рассмотрим случаи где может быть 1 валентная, 0 валентная????
Значит в s' есть и 0 и 1 валентная.

1 в s => если из 1 валентное доставляем сообщение, она остается 1 вал в s'
Если за границей то туда мы могли прийти из s' по .


Лемма 2.2.
Найдется такая конфигурация что в ней решится судьба вернется 0 или 1. Доказательство. 


### Семинар 3
Теорема. (повторение  из лекции).
faults < n/2
Не различаем partition и падение узлов.
Слева 3 node (A) справа 2 node (B)
При потере сети Nodes справа буду тупо бесконечно ждать. 
То есть хоть в какой то части должно быть нарушено свойство termination.
faults < b или faults < a <=> faults < max(A,B)
minimize(max(A,B)) -> A~B~n/2. 
n = 2k+1  A = k +1 b = k
n = 2k a = b=k
f < n / 2.

Алгоритм консенсуса не может завершиться даже если f >=n/2.

FLP теорема (CAP теорема consistency, availability, partitioning - нельзя построить распределенную систему, в которой выполняются все 3 свойства.)

FLP на практике бесполезна, а теорема о n/2 на практике учитывают.
Так как в FLP говорится, что если случился 1 отказ то сценарий как там теоретически может случиться. И по факту такое бывает редко 

Dead lock и Live lock - программа что-то делает вычисляет, но по факту полезная работа не выполняется.
При нарушении FLP будет livelock, при нарушении n/2 в системе все вообще плохо.

FLP повторение доказательства. 

У каждой node: in memory per host, disk per host, outgoing message. Назовем это конфигцрацией.
Строим граф конфигурация. Исходящих ребер максимум количество сообщений в сети. 

 
Рассмотрим исходное для S состояние. Они бивалентно. 
Значит в S есть и 0 и 1 валентные. Дальше если в S' есть 0 состояние или 1 состояние.Дальше см переходы из S. Провести эти рассуждения. 

![Brief scetch of last steps](img/20240928_092217.jpg)

И в S0 и в S1 могут быть бивалентные состояния. Говорим что S0 - переход в 0 по (h,m), S1 - в 1.
Всегда существует путь из S0->S1. Почему? Стартовое 0/1. Пусть этого пути нет. Пусть стартовое лежит в S0. Как то же оно должно попасть в S1. А вот он и путь. 

Если хосты разные отправляют разные сообщения, то порядок нам не важен. 
Значит h = h' Тогда все решает хост h. 
Крутимся через самое старое сообщение в сети.
Зачем самое строе сообщение из сети берем? Если будем брать не самое старое то некоторые сообщения будут лежать вечно. Из-за этого там могут лежать сообщения разных хостов. А мы переживам только 1 отказ(только сообщения 1го хоста могут лежать вечно). 

Мы ожидали 1 отказ. А мы построили 0 отказов и при этом имеет live lock. Где 1 отказ применяется?
Отказ нужен не для исполнения, он нужен что бы алгоритм **был бы к нему готов** и алгоритм выполняет какие-то шаги рассчитывая на то, что отказ возможен. 

Распределенные файловые системы. DFS

GFS - Google file system. Первая версия. Colossus.  

syscall. open, write,read,move,remove.
В обычной FS есть inode, хранящая информацию о файле, здесь тоже будем иметь inode/
DataNodes где мы хранимы данные.
Файлы будем разбивать на блоки - chunk. Что бы хранить в разных местах. 
Коэффициент реплекации - RF. В скольких нодах храним chunk. Начнем с RF=3
Пишем на 2 ноды, на 3-ю дореплицируем потом репикатором. 
Replicator - дореплицирует недостающие chunk если количество записей меньше RF.
Chunk - Node ids, RF, 
Inode - Chunk ids, path, creation date, id. 

PrimaryMaster - RSM

Master-Slave replication
Chunk immutable в современных системах. И не фисированного размера. 
Хотим append в файл. PM находит nodes 

Merger.  Условно слишком маленькие chunk будет объединят в нормального размера.

![Scetch of GFS](img/20240928_095921.jpg)

### Лекция 5
Продолжение семинара.
Есть много data nodes.
Есть file, состоящий из chunk.
Хотим в файл уметь append, read,move,open,remove.
Есть  еще pm(rsm внутри).
```
Open = pm : ReplicatedOpen(request)
	if file  does not exist
		create file
	return descriptor
```

Append
1. client->Pm.open(),
В Pm не пишем данные, будет слишком нагружаться.
1) build chunks
2) store chunks
3) update chunk ids(in file)
Разделение на chunks можно встроить в клиентскую библиотеку. Не лучший выбор, так как при желании обновления chunk придется обновлять всех client, но можно начать с этого.
Лучший способ - использовать некоторый proxy, где будет наш код который мы всегда сможем обновить. Ходить к Pm/  data nodes будем через proxy. (Пока не рассматриваем.)

Заведем SecondaryMaster(SM тоже RSM внутри) которые будут отвечать за chunks и data nodes.(Какой chunk лежит на каких nodes) (PM знает про дерево файловой системы).

2. client -> Sm  creat chunk
3.  client->data nodes store chunks
4. client->pm update chunk ids

Есть две стратегии информации о chunks в SM. Вторая  : data nodes хранят инфу про то,какие они chunks хранят и периодически отсылают  эту инфу в SM. 

Итак, в SM храним struct chunk,в котором RF, chunk id, node ids.
```
Replicated Append(request)
	File chunkids.append(request.chunkids)
```
Нужно еще учесть:
После restart проверять валидность данных.


Datanodes <->SM посылают раз в n секунд heartbeat.
SM:
Transient state(in memory) : chunk, node -> last alive time
Persistent state(RSM): chunk->node ids, rf
Heartbeat обновлять будет Transient state.

Current rf можно вычислить по списку node в transient state.

Шардирование. Данные, разбитые на маленькие блоки назовем shards. Нужно что бы не искать в 1 м большом чем то, а разбить на несколько и искать в нескольких.
В целом, SM может быть шаридированным. (PM не может быть. Не понятно как шардировать дерево).

Шардирование и heartbeat - то почему нужны SM. PM  бы не вывез.

Replicator. Как делать репликацию. Делать надо в SM. 
1. Under replicated chunks
2. Pick chunks for replicator
3. Pick alive node <- source target node
4. Send replicate request to source node 

Может ли быть 2 replicate? Если юзать чисто transient и узлы падают, то да.
Overreplicated chunk тоже бывает, от них стоит избавляться тоже.

Диапазон между retry.
Jitter(случайная величина) и backoff(постоянная) определяют это время ожидания.

Консистентность append. 
Вариант 1 При открытии брать lock .На момент записи проверять есть ли lock.
Вариант 2 Механика prerequisite timestamp. Каждое действие PM будет снабжаться  timestamp. 
В File будем хранить Modification  timestamp 

Read return file mts (FMTS)
при append будем слать (FMTS). Если такой же, то файл не меняли в промежутке. А если нет, то кто то менял и чанки не валидны и надо заново делать append. "оптимистичная блокировка"

Делать retry read, так как обычно  write меньше и вклиниваний будет  мало. 

Move происходит строго в памяти PM. Так и все операции над директорией происходят чисто в PM.


### Семинар 4

gRPC
Protobuf

### Лекция 6 
Повторение DataNode,primary master. 

Map Reduce. 

Данные хранятся в виде таблицы 
table = list of rows. Row = map\[name_col\] -> value.
table = list (key,value). 
Просто 2 способа говорить об одном и том же 

Операция map(row) -> row\*
map(key, value)->\[key\',value\' \]

Операция reduce(key,\[value\]) -> \[value\]

Пример. Посчитать количество слов в таблице. 
Утверждение. Практически все вычисления можно делать в терминах map/reduce.

ps -efl | awk '{print $3}' | sort | uniq -C
		map                       reduce

Inner join. Декартово произведение пересечения двух множеств. Брем 2 входа и объединяем по ключу. 

Реализуем Hadoop v1.
Параллелим map. Разбиваем просто input на части. Передают машины все в shuffle + sort облако.
1. Приходит клиент с job = \[tasks\], input, output, mapper, reducer, num of map nodes(M), num of reduce nodes(R). Task = map/reduce. Приходит он в некоторый jobTracker.
2. jobTracker идет к PM за дескриптором в DFS
3. Дробит input на M частей и отправляет на map nodes. Сама выбирает какие то nodes  куда отправлять. Сама поддерживает связь через ping с этими map nodes.
4. Map. Накапливает результат в R штук buffer, которые лежат в памяти map node. Partition - buffer для конкретного reducer. Как понять в каком partiotion?  hash(key) % R. Если buffer большой кидаем на диск. Но перед этим вызываем на нем sort.(external algorithms)
5. Хотим на каждую partition теперь 1 файл. Сделаем merge по partition merge sort. Получаем ровно R файлов в ЛОКАЛЬНОЙ системе map node которые внутри отсортированы.  
6.  Shuffle
7. Sort
8. Map после завершения говорит jobtracker результат операции map, когда тот завершился. 
9. Так, Reduce node вычитывает partition из той map node, которая это считала. Reduce node получает это знание из того что ему jobtracker послал. Jobtracker знает, так как map по завершении ping об этом.
10. Результат reducer записывет в DFS. (будет R файлов) 

+:
map, reduce распараллелены. 
Используют HDFS. input имеет некий RF. Так как файл уже лежит на многих машинах попробуем запустить map на тех машинах, где файл уже лежит. Таким образом оптимизируем сеть. 
Locality (оптимайз сети)
Combine (оптимайз сети) 

Fault tolerance. Если падает map или reduce переделываем только task.  Но стоит делать limit. А то так можно получить бесконечный цикл, поскольку пользователь может дать некорректный код. 

Фикс падения job tracker - иногда скидываем в  локальную память(а можно и в DFS) state of tasks. 

Speculatie execution. (+time - cpu) Посылаем на несколько node одну и ту же задачу. Выгодно, так как некоторые nodes быстрее некоторые медленнее.  

Записи в локальную FS : 1 сброс buffer + merge + reducer merge. При записи на DFS можно сделать лишь 1 запись. Объединение файлов через указатели.

R < M. 
DFS А также HDD лучше писать больште куски. Пожтому R лучше не делать маленьким. 
Ограничений на M особо нет. 

Hadoop v3 YARN
Решили для 1 го клиента = 1 job. Как решить для нескольких клиентов = несколько job?
Введем AppManager, который следит за jobs. 1 job = 1 appmanager. Идет к resource manager и спрашивает какие узлы доступны. Resource manager следит за тем, какие node работают.  



### Семинар 5




### Лекция 7 
Raft consensus


### Лекция 8
Кр: задизайнить что-то плана отказоустойчивого сократителя ссылок. (книжный пример, будет что то иное)

Odered K-v storage persistent local. Отказоустойивость на 1-й машине.
- put(key, val)
- del(key)
- get(key)
- scan(low, high) -> iterator (Получить все ключи в этом интервале)
Хотим параллельность всех операций.
- Операция Snapshot() -> k, v storage read only. Get и Scan переходят сюда.
Persistent = переживаем перезапуск, хранить будем на диске.
В памяти можно сделать std::map / другие деревья.
В map мы обходим дерево, если память, то это доступ в памяти и это ок. А вот когда мы захотим перенести в диск, то придется постоянно читать с диска, что как известно не здорово. Тогда одно обращение ~2-10мс для HDD. 
Выгодно писать последовательно много. 

SSD.  Набор блоков, блоки разбиты на страницы. Построен на флэш памяти. FTL.
read(page)
program(page) - set 1's to 0's. Только применимо  к чистой (состоящей только из 1)  странице.
erase(block) - set all block to 1. 

Пользователь записал какую-то страницу. Хотим ее изменить -копируем страницу в новое место и меняем на ходу. Страницы ~4Kb. В фоне garbagecollect которые отслеживает состояние блоков и если надо делает erase. 
SSD выгодно что бы писали что-то новое, что бы не перезаписывать данные. Выгодно что бы писали последовательно - пишем целый блок. 

Для того, что бы map работало в памяти, хотим писать последовательно много и что-бы записи были новые. 


B - дерево. В node храним пачку значний K -v. и  B nod на уровне. Выгодно, потому что высота сильно меньше. То есть когда будем обходить дерево, меньше чтений получим. 
B+ дерево. Храним только ключи в node. Значения в листьях. Все node соединяем в список. 

Быстрый Get.
Разобьем сортированный файл на блоки и у некоторого index будет key, offset. Табличку index хранится и на диске и в памяти.  Бинпоиск по ключам (в памяти). нашли offset, вычтем начиная с offset данные. И уже там найдем ключ.  Оптимизация чтения с диска с log раз до 1 го. В index key, offset. Так как мы знаем offset то unzip этой части. 

Быстрый Put (k, v).
Храним некий Log - очередь. 
в Log просто кладем всю операцию (put(k,v)).

Тогда get будет искать первую запись с конца. Будем хранить в памяти MemTable, который будет запоминать последний put в этот ключ. Тогда get ходит только в memtable, put пишет и в log и в memtable.
Когда memtable большой скинем его в файл на диск, как мы делали в get. memtable -> dump -> sstable. sstable - тот сортированный файл и таблицца с индексами. sstable будет несколько. К тому же можем еще и zip делать. Чтение будет всего одного диска, по этому 1 unzip ничего. Теперь можем и log удалить, так как все это в sstable. Файлы sstable связаны между собой. 

Static to dynamic. Файлы размера степени двойки, делаем merge когда у 2-х файлов совпадает размер. Compaction.

Delete (k) = put(k, flag). 

Получаем 1 запись = 1 чтение с памятью.  

LSM log-structures merge-tree.

Фильтр Блума. Храним в памяти. 
set[]
- contains(k) -> No\Maybe 
- put(k)
Будем иметь несколько хэш-функций {h}. 
Put(k):
h1(k), hn(k) - позиции в массиве. На эти позиции ставим 1. 
Contains:
h1(k),hn(k) - Если попали в хотя бы в один 0, то точно знаем что нет элемента.
tradeoff error vs size

Тогда sstable = sorted file + index + filter и на диске. index + filter и в памяти тоже. 
При поиске ключа в sstable вначале проверим в sstable.  


Как устроен Memtable. 
Skiplist. Отсортированный list с некоторыми еще указателями вперед.
С какой то вероятностью продолжим добавление на уровень вверх. 
На put и get будет o(log ) как в treap. 

Реализация LSM - levelDB google, rocksDB. 

Всегда новая запись будет в такой реализации.  Только append. Значит можно это делать и в DFS.  

Snapshot нужен для get и scan. Он использует  memtable. Можем просто делать персистентый skiplist. Или же на каждый put делать  fork, так как там COW. 




raft, lsm, flp, mapreduce




## Лекции мфти 
### Лекция 1 

Очень разные типы распределенных системы:
- KV Storage 
- File System
- Coordination service ~atomic
- Message Queues
- Databases

Зачем нужны распределенные системы?
- Горизонтальная масштабируемость 
- Отказоустойчивость
- Проверка корректности данных, если не доверяем каким-то хостам.


Мы ждем от системы гарантий.
Пользователь, node- часть системы.

Message Passing. Внутри системы 
Shared memory. Снаружи системы.

Связь между node обозначим просто проводом. Почему?
Fair-loss link. Не гарантирует доставку. ->
Reliable link. Построим как tcp. по дублированию через id пакетов, по retry до получения Ac.
Вот здесь, мы считаем, что соединение или гарантирует доставку или порвалось. 

Время доставки сообщений:
Асинхронная - нет какого-то конкретного ограничения на время доставки.
- Safety свойство. Не делает плохо.

- Liveness свойство. Иногда делаем хорошее.
Частично-синхронная - существует точка t когда происходит переход safety- liveness.

Partition. Split brain. Система разбивается на две части. Да еще и по разному отвечает и работает.

Сбои node:
- Crash. Умирает и не восстанавливается.
- Restart. Перезагрузка узла 
- Византийские соглашения. Ведет себя хаатично.

Время. 
Надо считать timeout для failure detection а так же делать ordering. 

Для синхронизации часов лучше чем (t2 - t1) / 2 не сделать. То есть сеть работает симметрично. Т к можно доказать что алгоритма синхронизации не существует.

GPS в реальности синхронизирует еще и часы, трех пространственных координат мало. Навигационные  уравнения GPS.  

Google cloud Spanner. Реализуют TrueTime, где TT.now()  возвращает интервал \[e, l\], где настоящее время гарантированно лежит внутри этого интервала. TT.now() работает **ЛОКАЛЬНО**! Устроено это так, что сервера(Time master) синхронизируют время со спутниками GPS. А другие сервера(Armagedon master) содержат в себе атомные часы. И тогда происходит так: в момент времени t\* мы синхронизируется часы, получаем \[e, l\]. Когда через d нам приходит запрос мы возвращаем уже \[e + d \* const, l + d \* const\]. Где const - некоторое количество ppm, заведомо большее чем дрейф локальных часов. Но за счет частой синхронизации, интервал не расширяется на очень много.

### Лекция 2 

Рассмотрим KV Storage 
- Set(key, value)
- Get(key)

Репликация Register. Т.е. как хранить данные, помещающиеся в 1 машину надежно на разных.
Реализация: 
Конкурентная история : (то как в реальности приходили разные запросы к разным узлам системы)\
//////// w1
   ////////// w2
      /////// r
Последовательная история w1 w2 r w3 r. (Как мы пытаемся объяснить себе порядок выполнения)
У регистра есть несколько спецификаций - возможных последовательных историй.  

Модель согласованности - отвечает на вопрос "А какие конкурентные истории может порождать конкретная реализация для заданной спецификации?"

linearizability. ~External consistency
Для любой конкурентной истории существует последовательная история такая ,что, если в Конкурентной истории  одна операция O1 завершилась физически раньше другой O2, то и в последовательной будет сначала O1 потом O2. (Real time ordering)

Если система линеаризуема, то можно думать, что операции типо атомарные и выполняются как-бы **одна за одной**.
Это как раз про то, что операция выполнилась в какой-то определенный момент времени. До нее, операции не было, после она уже выполнена.

Atomic Register.
Single writer. 3 хоста.

Добавим к write timestamp, что бы понимать актуальность данных. Так ка writer 1, то timestamp он просто ++. Write(val, timestamp)
Write, отправляем запрос на все 3 хоста, ждем ответ от 2-х, пишем на 2. После полной записи на 2 хоста всегда отправляем AC клиенту.

Read.  Отправляем всем 3-м. Ждем ответа от 2-х. Получаем max(ts1, ts2).

Обобщаем 3 хоста до 2N + 1 хостов и вводим систему кворумов = N / 2 + 1.

Могут быть асинхронные кворумы, что бы одну операцию делать медленнее, а другую быстрее. Так, что бы увеличить чтение мы можем увеличить кворум на запись, но уменьшить на чтение.

Но если write еще пишет, а нам пришел read, то мы не знаем, про состояние write. Что бы следующий read прочитал как минимум то же что и мы, то после read соберем кворум на запись того, что прочитали.

В линеаризации выстроим все в порядке timestamp и он будет согласован с реальным временем. 

Multiple writer.
Надо выбирать timestamp распределенно монотонно. 
Фаза write:
Соберем кворум на чтение,  достанет max timestamp, возьмем себе timestamp + 1, соберем кворум на запись с нашим новым timestamp.
(может быть еще запись write в процессе, т е делать +1 может не сработать)
Commit Wait. timestamp можно выбрать не собирая кворум на чтение максимальной записи, а просто подождав некоторое время чего-то. 

### Лекция 3 A-Broadcast
Общая задача - построить распределенный DataBase.
Есть Local Storage. Будем реплицировать(дублировать) данные на другие машины. Сверху на KV уже будет SQL.

Linearizability. 
Если в реальности одна операция выполнилась до начала другой, то и в линейном порядке их порядок будет в том же отношении.

Повторение про Multiple writer, single register.

Добавляем операцию CAS.
Проблема. Нужно хорошо понимать значение ячейки, и это должна понимать каждая реплика, а они не синхронизированы. Для того, что бы эти реплики сходилась к 1му значению мы используем timestamp. 
 
Когда мы просто читаем, пишем, то если нам приходит запрос с более старым timestamp,  его можно просто отбросить и забыть про него. А в CAS так нельзя. Значит нужен другой алгоритм.

 Хотим, что бы тот порядок, в котором команды приходят на реплику и был порядком их применения. Да еще и так, что бы этот порядок **у всех** реплик был одинаковым. Это реализует Atomic Broadcast.

Atomic Broadcast:

A-Broadcast(m). Отправляет всем сообщение m. Гарантирует, что рано или поздно на каждом узле, в том числе на исходном, вызовется обработчик.
A-Deliver(m). - Обработчик.

Свойства, которые мы требуем:
Validity - Если несбойный узел запускает Broadcast, то он вызовет обработчик на самом этом узле
Agreement - если какой то несбойный узел доставил сообщение, то и на остальных узлах это сообщение будет доставлено.
Total order - Общий порядок доставки для каждого узла. 

  Для любых двух узлов префиксы доставленных сообщений должны совпадать.
  --------x-----y
  ------x-----y---
  -------y crash node. Такое нельзя.

Рассмотрим State Machine. Автомат с состояниями, состояния переключаются.
Используя A-Broadcast() будем переключать состояние. Когда приедет на узел,  переключим состояние. 
Atomic broadcast как просто транспорт команд. 

Кворумы внутри реализации AB.

Линеаризуемость. Agreement + Total Order => такого быть не может.
Обработчик 2го сообщения на узле 1 должен быть вызван раньше чем обработчик 1го сообщения. Тогда сообщение должно было быть отправлено до момента его отправки 2м узлом.

Важное замечание - операция которая происходит в автомате, которую мы всем узлам даем должна быть детерминированной. (Хэш таблицы, время, дробные числа)

Еще одна проблема. Клиент может не получить AC так как нода умерла. По timeout мы его поймали. Retry мы сделать не можем, так как клиент не знает состояния системы, применилась ли команда или нет? Тогда может вознинуть повторение действия команды. 
Семантика Exactly ones. 

RSM много. Внутри есть Total order. Но снаружи, на всю сеть RSM этого total order нет.

#### **Реализация** Atomic Broadcast.

^fd9023

1. Сообщения нужно доставить
	Reliable broadcast.
		Гарантирует:
		Все сообщения, которые были отправлены корректными узлами будут получены всеми корректными узлами. и все получат один и тот же набор сообщений.
	Отправляем всем свое сообщение. Когда какой-то узел получает первый раз новое сообщение он его отправляет всем. 
	Все. Это и позволяет соблюсти гарантии.


2. Сохрнить порядок сообщений.
	Consensus.
	На каждом узле нам нужен алгоритм propose. На вход дается input. Узлы все вызывают Propose(input). На выход получают некоторый общий out. Этот out должен быть: 
	Из предложенных значений(Validity),
	 Единый для всех узлов. (Agreement),
	 Алгоритм должен завершаться (Temptation).
	
	Будем делать серию консенсусов. 
	A-Broadcast =R-Broadcast
	A-Deliver(m) = R[] += m (Добавим сообщение во множество сообщений, доставленных RB). Для них обработчики доставки еще не вызваны.
	A[] - множество сообщений, для которых обработчики доставки уже были вызваны.
	Фоновый процесс:
		Раунды. r = 0
		while R[] - A[] != 0 (пока есть сообщения, которые еще не были доставлены )
		r = r + 1
		Bi[] = Propose(r, B[]) Предлагаем на консенсус те сообщения, что были нами получены но еще не отправлены R[] - A[].
		На всех сообщениях из Bi[] вызываем обработчик доставки 
		A[] += Bi

	Все. В каждом раунде доставляем какое-то количество сообщений. Все зависит от Propose.

Как решить consensus с помощью AB?
Пусть каждый узел в propose выполнит AB и выберет то, что вернет ему первый обработчик доставки. 
Но мы хотим наоборот AB через consensus и reliable broadcast.


Итого 
RSM > AB > Consensus.
RSM - хорошо, сможем мысли о системе в которой несбойные надежные узлы , но нужно научиться делать Propose.


### Лекция 4 FLP
Невозможность консенсуса, FLP.

По факту AB ~ Consensus. Consensus -> AB (Propose - каждый узел посылает A-Broadcast и выбирает первое значние, которое ему было доставлено.)

Лемма 2. Из бивалентной конфигурации можно перейти в бивалентную конфигурацию. 
Пусть есть C - некоторая бивалентая конфигурация.(При этом ни один узел не был потерян) Есть некоторое множество S - то множество, куда мы можем прийти не отправляя сообщение (m, p).
Но рано или поздно мы должны отправить это (m, p). Подрисуем снизу это множество.
Утверждение. В этом нижнем множестве Se тоже есть бивалентное состояние. Доказательство от противного. 
1. Если в Se есть только унивалентные конфигурации, то там есть хотя бы одна 0 валентная и хотя бы одна 1 валентная. Так как C по условие бивалентная, а через Se мы обязательно пройдем, в Se есть и 0 валентная и 1 валентная.
2.  Утверждение. В S по переходу (m,p) мы попадем в 1 валентность, а по переходу (m', p') и потом (m, p) попадем в 0 валентность. Почему? Пусть цвет исходной вершины С (пусть красный) определиться переходом (m, p) - то, в какое состояние мы попадем если сразу применим переход (пусть 1 валентное). Но! Есть же  состояние другой валентности (зеленое 0 валентности). И есть какой то переход из S в Se зеленое (по (m,p)). Тогда получаем это ребро то, в котором меняется цвет. Получаем, если соединить C и то, откуда мы перешли в зеленое, они будут соседними.  То есть от порядка зависит то, в какие валентности мы попадем.
3. 1.  p' != p Такое невозможно. Почему? По (m,p) попадаем в 1 валентное. Если потом проделаем (m', p') Попадем в 0 валентное, а такое невозможно. 2. p' = p. Состояния C', C'' отличаются только действием p. Убьем узел p. Так как мы должны уметь переживать хотя бы 1 отказ, то найдется какой-то другой путь s который приведет в определенную конфигурацию (пусть 1 валентную). Но! Мы в праве этот путь прикрепить после сообщения (m,p) и (m', p) так как в нем не задействован этот путь s. (m, p) и (m', p) ведут в РАЗНЫЕ валентности. S ведет в каккую-то конкретную. Тогда будет или переход из 0 в 1 валентную или из 1 в 0 валентную по s. Противоречие.
Тогда лемма 1 +  бесконечное число раз лемма 2(сообщения используем в очереди самое старое из недоставленных) = отсутствие Termination. 
Получаем LiveLock.




### Лекция 7 RAFT
Raft. Алгоритм консенсуса. 

Задача репликации log. 
На 1-й node есть RSM, consensus module, log. 
Выбирается лидер. Без лидера алгоритм не работает.

Декомпозируется на 2 типа: выбор лидера и репликация команд при стабильном лидере.

Есть 3 роли: Лидер, Follower, кандидат. Роли взаимоисключающие. 

Время распределено на terms. В terms есть максимум 1 выборы лидера в начале terms и нормальная работа если выборы были успешными.

У каждой реплики своя вот картина terms. Она могла быть выключена и не знать о том, что происходило в системе. 

Heartbeats. 
Followers  поддерживают таймер electionTimeout и обновляют его, если от лидера пришел heartbeat. Если же не получили такого за timeout, то followers считают что лидер умер и начинают выборы нового лидера.

#### Выбор лидера. 
Реплика у которой истек таймер повышает свой term на 1, меняет статус на кандидата и отправляет на остальные ноды запрос на голосование. 
Реплика голосует за первого кандидата в term и запоминает его. 
Тогда будет 3 варианта: или проголосовали за этого кандидата, или выбрали другого(пришло  сообщение, которое только лидер посылает с нашим term) или истек timeout(снова term+1 и повторяем).

**Safety**: В таком сценарии не может быть больше 1го лидера в term. Почему? Каждая реплика голосует не более 1го раза за term. 
**Liveness**: Electiontimeout будем выбирать немного рандомно. (Все ноды изначально followers. Поэтому они  могут просыпаться одновременно и  начать голосовать все только за себя.)  Тогда случится момент, когда 1 сервер проснется и проведер выборы раньше других и все проголосуют за него. 

**Настоящий способ голосования за лидера**
1. В том term мы еще не голосовали
2. Последний term в log у кандидата БОЛЬШЕ или равен, но размер лога у него больше чем у нас 

#### Репликация log.

Log - последовательность слотов с командами и номер term в котором ее положили. Log храним на диске, чтобы переживать рестарты. 
Все операции с log меняют его suffix. Лидер log только добавляет и реплицирует.

Инвариант. 
Если на двух репликах в слоте по одному и тому же индексу лежит запись с одним и тем же term. То это означает что там совпадает эта команда и все префиксы до этой команды.

Лидер при отправке appendentries будет отсылать инфу о предыдущем состоянии и term. Если они не совпадают, то видимо, нужно чинить log у follower'а.

Если у follower там пусто, то будем искать последнее место где была запись, это нам скажет эта реплика. Реплика просто отстает.
Если там есть запись, надо искать такой префикс, что бы реплики совпадали(будем откатываться по степеням 2), причем будем стирать log у follower.

Так как лидер стирает логи других, лидера будем выбирать чуть по другому, аккуратно.

#### Когда команда надежно зафиксирована(закоммичена)?
1. Если ли лидер в терме k положил команду на кворум, то она закоммичена.
2. Если команда придавлена сверху командой из пункта 1.

Из этого следует State machine safety. Если применили команду из закоммиченного лога, то эта команда не поменяется.  
Safety обеспечилось.

 Если не все ноды могут общаться со всеми, то могут быть проблемы. Так, heartbeat некоторые не будут доходить до работающих узлов и они будут вызывать election. Поэтому стабильное состояние не будет достигнуто.
 Решение, часть 1 фаза PreVote(проверка, способна ли node стать лидером).
Виртуальный requestvote. Реплика отвечает положительно, если ее устраивает состояние лога и если она не получает heartbeat от лидера.

Часть2. CheckQuorum.
Проверяет на  лидере, что он вообще собирает кворум.




### Лекция 9 
Google colossus. DFS.





### Семинар 1
UTC - добавляем високосные секунды, что бы время совпадало с наблюдаемыми физическими явлениями.
GPS - в начале добавили немного високосных секунд, что бы синхронизовать с UTC с 1980х.
TAI - атомные, високосных секунд нет, меряем только по физическим процессам в атоме цезия.

Wall time  - Общая точка отсчета для всех node, но ведут себя не монотонно. Это как раз то, что не возможно синхронизировать
Monotonic clock. Разная точка отсчета для всех node, но ведут себя монотонно.
В c++ system_clock - wall clock
steady_clock - monotonic_clock

Leap smear - исправление високосной секунды. 
То есть секунды в компьютере будут идти чуть чуть медленне, что бы убрать эту разницу. 

Single point of failure. Единая точка отказа. Делаем систему так, что бы вот этой точки отказа не было. Что бы потеря какого-то одного оборудования не обрывала бы работу всей системы.

Много маленьких кластеров - не выгодно. Делаем один гигfнnский кластер, что бы было много связей.

Даже у TCP есть проблемы. У client мы не видим полной картины. Сервер может перезагрузиться, выключиться, а tcp подключение на client'е может остаться. Это проблема, это не чиниться, это надо учитывать.
Мб [тут](https://gitlab.com/users/Lipovsky/projects) можно поискать задачи/реализацию.


### Семинар 3. Local Storage, LSM.
- Put(k, v)
- Get(k)
- Delete(k)
- ScanRange(low, high)

Гарантии к бд ACID.
Atomicity. Если запись была начата, а в этот момент машина перезагрузилась, то запись или полностью записалась или полностью не записалась.
Durability. Если запись была подтверждена, а потом машина перезагрузилась, то данные сохранны. 

Hdd. 
Блины, на них дорожки, на дорожке сектора. Сектора по 512Kb. Есть считывающая головка. 
Что бы перейти к другой записи нужно передвинуть иглу на дорожку и доехать до нужного места. 6ms на оборот, в среднем 3ms на доехать до места. А еще seek. В итоге 5-10 ms на поиск. В итоге последовательно все работает гораздо быстрее. 

Ssd.
FlashMemory.
Ячейка->страница->Блок. FTL.
- Read(page) ~10ns
- Programm(page) Переводит все 1 в 0.
- Erase(block) Поставить все в 1.
То есть перезаписывать мы не умеем. Нам выгодно писать в новые страницы. Сборщик мусора помечает старые не валидные страницы в фоне. Но если мы не пишем последовательно, то потом придется двигать данные, что бы освободить место. То есть, из за того, что мы умеем чистить весь блок целиком, валидные данные из блока придется перемещать в другой блок. А вообще, число перезаписей блока ограничено. Поэтому писать последовательно все равно выгодно и нужно. 


Map = Красно-черное дерево, но работает, но требует слишком много доступов в память. Будем использовать B дерево - не 2, а больше ключей в node.  Глубина меньше, значит доступов в память меньше.
B+ дерево.



LSM. Log-structured merge tree.
1. Делаем Get(k).
	Sorted string table (SSTable). Храним отсортированный файл. Делим его на блоки с одинаковым размером offset, у каждого будет некоторый key. Создадим еще таблицу Index с  key->offset. При get найдем по этой новой табличке offset и перейдем в ту часть файла.
2. Быстрый Put(k, v).
	Поддерживаем файл на дискеLog. когда приходит put просто добавляем в конец put(k, v).
3. Быстрый Get(k).
	MemTable в памяти = часть Log. Put = Put_log + Put_memtable. Когда MemTable переполняется кидаем его на диск в SSTable. Итак, в памяти несколько сжатых SSTable с последовательными ссылками друг на друга. Причем, эти SSTable мы будем мерджить в размеры степени двойки, что бы каждый следующий был в два раза больше предыдущего.
4. Delete(k) = Put(k, flag). Когда в фоне будем сливать два SSTable если есть более новое значение с flag то просто будем его и сохранять, запоминая, что этого значения как бы нет.
Оптимизации:
1. BlockCache. Популярные блоки будем хранить там. 
2. BloomFilter.Добавляем его в каждый SSTable. Множество с операциями Add(x) и Contains(x). Причем  Contains отвечает No\Maybe. Берем битовый вектор и k хэш функций. 
	- Когда вставляем ключ вычисляем хэш функции и в те биты ставим 1.
	- Когда ищем x считаем k хэш функций и смотрим на биты, если хотя бы где то нет 1, то ответ NO.
3. MemTable. Реализован через Skiplist.


### Семинар 4. DFS, GoogleFS.

Можем не думать про отказы дисков. Получаем такой PersistentDisk, который выживает при отказе машины. 

API:
- Create(p)
- Delete(p)
- Copy()
- PRead(p, offset, size) - запись в файл по некоторому оффсету. 
- Write(p, offset, data)
- Append(p, data) - offset выбирает сама система. 
Колоссальное отличие в системе будет в зависимости от того будет ли мы использовать write + append или только append.

Обойдемся append.

