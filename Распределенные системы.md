### Книги по рекомендации Бидзили
Introduction to Reliable and Secure Distributed Programming - теория
Designing Data-Intensive Applications The Big Ideas Behind Reliable, Scalable and Maintainable Systems - практика
[Плейлист](https://www.youtube.com/playlist?list=PL4_hYwCyhAvaYKF6HkyCximCvlExxxnrC) Романа Липовского для мфти
## Лекции в МИФИ
### 04.09.24 Лекция 1
Иерархичность серверов => Главные меняются. Clos архитектура.
Система хостов полносвязана, имеет локальные диски. Общих данных нет, только обмен сообщениями.
Клиент как именно часть распред системы.
- Отказоустойчивость
- Не хватает одного хоста (вычисления/память)

Задача "консенсуса" как самая сложная в распределенных системах.

Задача: (похожа на  nosql )
key-val storage 
set(k, v); get(k) -> v.
Подразумевает асинхронность, но ответ даже на set необходим обязательно.
Лучше не отвечать, пока гарантированно не записал, у себя и желательно в остальных хостах.

Игнорим ошибки памяти(византийские гарантии),cpu, шанс что диск записал не то (check sum).
Host: 
- crash free;
- crash stop - сервер не реснется; 
- crash recovery может восстанавливаться и умирать постоянно. или полностью умереть как в crash stop.

Сеть - пакеты:  пусть нет corrupt, но может быть drop, а также duplicate. 

Fair loss  - рано или поздно сообщение будет принято

Perfect link - идеальная модель в которой все доходит 1 раз. 
Но даже в ней с умирающими хостами приколы.

Fairloss + tcp/ip = perfect link в достаточных для практики  пределах
Работаем с perfect link но не смотря на это помним про проблемы 

1.  Отказоустойчивость: не смотря на смерть хостов данные будут сохранены  
2. Масштабируемость (вплоть до линейной)
3. Консистентность

~40 шт хостов - стойка. Обмен данными к хостам через общий switch.
Тогда важно писать в хосты на разных стойках.
Иерархия доменов отказа.

round trip time 1-3 ms в data center 

Два вида свойств/гарантий в распределенной системе:
1. Safety Система ведет себя или корректно или нет.
2. Liveness . Три режима ответа сети. О времени ответа сети,  том, когда будет полезный ответ. Асинхронная, синхронная, частичная. Частичную рассматриваем. 

Время и часы.
Дрейф часов. Кварцевые ~1.7 sec/day. Атомные. Можно сказать не дрейфуют.
Но синхронизация часов тоже требует время.
Физические часы:
wall clock - аппаратные. Синхронизируются, есть високосные секунды.
monotonic clock - Локально. С момента старта. Никогда не откатываются.
Логические часы.

не можем синхронизировать больше чем eps.

### 07.09.24 Семинар 1
Сем. Ссылки с него 
Некий [userver](https://userver.tech/)
Описание про [thread](https://wiki.osdev.org/Thread)
[Примеры](https://github.com/chriskohlhoff/asio/tree/master/asio/src/examples) асинхронности, синхронности сервера



### 11.09.24 Лекция 2
K-v storage.
Regular Register.
Write (val)
Read->val

Single write. 
3  хоста всего. Можно ли писать на 3 хоста? На 1 нельзя, так как он сам может умереть. На три нельзя, потому что хосты умирают и если будем писать на 3 при смерти хоста запись не будет выполнена. Пусть пишем хотя бы на 2 хоста всегда. 

Надо выбрать все три  и дождаться ответ от двух.

Читать с 1го хоста? Нет, может умереть.  Или может сохранять старое значение. 
Читать с 3-х хостов, нет так как если при такой системе умрет один алгоритм никогда не завершится.
Читаем с 2-х хостов. 

Соответственно пишем/читаем на 2 хоста.  Тогда если читаем, то обязательно пересечемся хотя бы с 1-м из тех хостов на который писали. Кворум. Читаем кворум, пишем кворум. 
Как определить где новые данные? **Timestamp**.Их генерит Single writer. Именно single, что бы можно было просто ++ на счетчик и ничего не синхронизировать. Логическое время.
Что-то типа requestId не работает. Так как они не упорядочены. Упорядочевать id в распределенных системах - задача сложная. 


В целом думают вначале так, что у нас есть лидер, потом решают как от этого избавиться т.е. как выбирать лидера.
| w1  |   |                         w2          |  
   |     r2   |
          |    r1            |
Такое возможно. Так как для  read2 может быть кворум:
 |         /    |  
 |2    /1/    |  
 |    |  | | |  | |   
|        / 1  / |  
     /

Write majority.
N хостов. 
2N -> N+1 пишем .  Fault tollerance  N - 1
2N +1 ->N+1 пишем. Fault tollerance N
Fault tollerance  - максимальное количество отказавших хостов. 

Кворум- любая система множеств в которой любые 2 множества пересекаются. 

Ассиметричный кворум - пишем на 4, читаем с 2-х. Типо чтение - чтение могут не пересечься. Используется когда очень много чтений.

Четное число хостов не уважают и считают что так не делают.
Так как кворум одинаковый, а Ft меньше.

Модели консистентности. 
Самая "слабая":
Eventual consistency  - когда нибудь все будет отлично и система придет к консистентному состоянию.

Самое "сильное" свойство.
Linearizability
Linearization point.
Операция произошла в какой то **определенный** момент времени. До него операции не было, после точки она есть.

Sequential consistency.
Разложение истории на временную ось без пересечений. 


Atomic register. 
Read Impose 
|read from quorum| impose |
impose = write  on quorum



### 14.09.24 Семинар 2
Процесс - единица исполнения, обладающая ресурсами(памятью)
Поток - -//- не имеет своей памяти 
На стороне OS и управляет OS через sheduller с вытесняющей многозадачностью.
FIber - кооперативная многозадачность. Работает пока сам не остановит себя.
Subroutine - просто функция  в ЯП
Coroutine - функция, что может быть остановлена и потом быть восстановлена с того момента, на котором была остановлена.

Stackfull / Stackless 

Callback генерятся сами внутри co_await 
В stackfull нет этих слов, там будет прям объект coroutine и его pure методы.

Примитивы синхронизации для fiber будут немного отличаться, ведь надо стопить определенный fiber, а не весь thread.

Courutine - при использовании co_wait, сама вставляет callback.

[Фреймворки](https://www.techempower.com/benchmarks/#hw=ph&test=fortune&section=data-r22) с fiber.

Userver
Почиать [документацию](https://userver.tech/)


### 18.09.24 Лекция 3
1 Writer -> multiple writers. 
Было у каждого write некоторый строго увеличивающийся timestamp. Порядок строился на них.
R  = R + W
W = R 
Собрали кворум на R из этого read получили timestamp потом делаем W (timestamp + 1)

Пришло два w по одному timestamp. Надо линеаризовать => упорядочить, но не важно как. Тогда добавим к timestamp у W  какой-то глоабльный уникальный ключ - GUID. И все, посортим по нему, ведь порядок должен быть любой. 

Доказательство линеаризуемости. 
    01
R + w         O2
          R + w
Кворум на O1 W и  O2 R пересекутся, получим из них timestamp. 
timestamp O1<=timestamp O2


Операция CAS. Compare and swap
```
CAS(k, v, v*)
	if (stor[k] == v){
	stor[k]=v*;
}
```

На какждом хосте должен быть один и тот же порядок что бы CAS работал корректно.
(Тогда CAS должен быть блокирующим для машины?)

Replicated State Machine.
	A replicated state machine is **a deterministic state machine where multiple machines have the same state and run the same**.
({stat}, {transition}). Transition - какой-то переход  внутри хоста по автомату.  
Хотим автомат разложить на все реплики в начальном состоянии? 
Хотим что б ы на каждом хосте операции прмиенились в одном порядке, что бы в конце концов все хосты были в одном и том же состояниию.
?????

Atomic Broadcast
A-Broadcast(m) <- отправить всем сообщение 
A-Deliver(m) <- обработчик
Свойства ^7c1e2a
- Validity, если не сбойный узел сделал  A-Broadcast(m) то он позовет A-delivery.
	Узел сбойный тогда, когда он умирает и не взаимодействует больше с другими узлами. Все остальные узлы не сбойные. 
- Agreement , если на не сбойном хосте позвался delivery(m) , то на всех остальных он тоже позавется. 
Получаем гарантию что все сообщения будут получены несбойными хостами. 
- **Total order**, Для двух хостов посмотрим их историю в любой момент времени. Тогда один из них является префиксом другого или наоборот.  

RSM с помощью Atomic Broadcast.
Broadcast оставляем.
Deliver - переключить состояние. 
При запросе: Broadacst, ждем deliver, при получении отправляем ответ клиенту.

Reg < Reg + CAS < RSM < AB < Consensus


Linearizability AB

То есть по total order другого порядка нигде быть не может, а значит и линеаризуемо.

Если узел координатор(к которому в начале пришел клиент) умер до отправки ответа клиенту, а такое может быть. То, это уже должен фиксить клиент. То есть если он не получил ответ, то он должен еще раз сходить в систему и сделать check или retry.// Exactly once нигде нет. 

Mutex = RSM 
Node &harr; Node

RSM умеет переживать перезапуски машин!

Propose(v) -> v\* 
v\* вернется на все хосты, причем $v* \in {v1,v2,v3}$

Validity - safety (на конечном промежутке)
Agreement - safety (на конечном промежутке)
Termination - Liveness ( на бесконечности)

AB:
Validity - liveness
Agreement - liveness
Total order - safety  


AB -> Consensus
Ab(m1), delivery вернет то, что типо должен вернуть propose
Ab(m2)
Ab(m3)

Consensus -> AB
Reliable Broadcast

Если сообщение пришло на хост первый раз то еще раз отсылаем всем и потом отправляем обработчик не дожидаясь ответа. 
AB: 
R-B(m) + R-D(m): Buf += m

Background: постоянно крутится процесс. 
Несколько раундов консенсуса. A = отправленные сообщения 
while buffer - A not empty{ 
propose(всех сообщений из buffer - A), вернет R\*
Ad(r\*)
}

### Лекция 4

~Кривое доказательство что cas не работает без state machine.

Алгоритм  работы AB. Смотри [[Распределенные системы#^fd9023]]

Задача бинарного консенсуса. $v* \in {0,1}$

Мы не отличаем partition от отказа узлов. 
При разделении посчитаем, что меньшая часть не будет выполняться. 
Тогда любой алгоритм консенсуса переживает $\frac{N - 1}{2}$ отказов.

[Теорема FLP.](https://neerc.ifmo.ru/wiki/index.php?title=%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%A4%D0%B8%D1%88%D0%B5%D1%80%D0%B0-%D0%9B%D0%B8%D0%BD%D1%87%D0%B0-%D0%9F%D0%B0%D1%82%D0%B5%D1%80%D1%81%D0%BE%D0%BD%D0%B0_(FLP))
Опр. Для любого алгоритма консенсуа который переживает хотя бы 1 отказ существует такое исполнение что алгоритм не завершится. 

Говорим что никогда не нарушаем Validity и Agreement. Значит можем нарушать Termination.
Доказательство. 

Конфигурация - состояние всех узлов + все сообщения в сети. Типо Snapshot. 
Есть направленные переходы между узлами: Доставили сообщение m.
Получили так граф конфигураций.

Исполнение - путь в графе = порядок доставки сообщений. 

Графом конфигурации описывается любой алгоритм консенсуа. Конкретное исполнение - конкретный путь. 

Бесконечное исполнение - бесконечный путь ИЛИ цикл.

Доказательство итеративное.

Лемма 1. ~Существует начальное состояние нам нужное. 
Для бинарного консенсуса. Каждая нода позвола propose.  Получается  $2^{n}$ начальных конфигураций.
Валентность конфигурации - то что вернет консенсус.
Конфигурация унивалентная есть мы знаем с каким значением завершится алгоритм. 
Конфигурация бивалентная если не знаем.

- 0 валентная

$$
R_{0} = \begin{pmatrix}
0 \\
\dots \\
0\end{pmatrix}
$$ 

- 1 валентная

$$
R_{n} = \begin{pmatrix}
1 \\
\dots \\
1\end{pmatrix}
$$ 

$$\Rightarrow \exists k : R_{k} \text{0 валетная, } R_{k + 1} \text{1 валетная,} $$
Они отличаются только значением k+1 хоста. Так как алгоритм переживает паделние 1 узла, то этот k+1 узел можно выкинуть. А значит оба этих узла бивалентные. 


Лемма 2. ~Всегда существует переход, который приводит в не завершение.
$$\forall (p,m), \forall c \text{ бивалентное }\exists \text{ путь } c \to c' \text{ что (p,m) , будет доставлен последним в этом пути и  }c' \text{ бивалентное }$$

Причем в этом пути не должно быть более 1 хоста умерло. 
S - множество конфигураций где (p, m) не доставлено
S' доставлено из S (p, m) последним.
А за границей этого контура что-то что после s'.
![S Sets](img/20240925_094206.jpg)

Лемма 2.1. Хотим доказать что в S' есть бивалентное состояние.
Докажем от противного. Пусть там только унивалентные. 
Рассмотрим случаи где может быть 1 валентная, 0 валентная????
Значит в s' есть и 0 и 1 валентная.

1 в s => если из 1 валентное доставляем сообщение, она остается 1 вал в s'
Если за границей то туда мы могли прийти из s' по .


Лемма 2.2.
Найдется такая конфигурация что в ней решится судьба вернется 0 или 1. Доказательство. 


### Семинар 3
Теорема. (повторение  из лекции).
faults < n/2
Не различаем partition и падение узлов.
Слева 3 node (A) справа 2 node (B)
При потере сети Nodes справа буду тупо бесконечно ждать. 
То есть хоть в какой то части должно быть нарушено свойство termination.
faults < b или faults < a <=> faults < max(A,B)
minimize(max(A,B)) -> A~B~n/2. 
n = 2k+1  A = k +1 b = k
n = 2k a = b=k
f < n / 2.

Алгоритм консенсуса не может завершиться даже если f >=n/2.

FLP теорема (CAP теорема consistency, availability, partitioning - нельзя построить распределенную систему, в которой выполняются все 3 свойства.)

FLP на практике бесполезна, а теорема о n/2 на практике учитывают.
Так как в FLP говорится, что если случился 1 отказ то сценарий как там теоретически может случиться. И по факту такое бывает редко 

Dead lock и Live lock - программа что-то делает вычисляет, но по факту полезная работа не выполняется.
При нарушении FLP будет livelock, при нарушении n/2 в системе все вообще плохо.

FLP повторение доказательства. 

У каждой node: in memory per host, disk per host, outgoing message. Назовем это конфигцрацией.
Строим граф конфигурация. Исходящих ребер максимум количество сообщений в сети. 

 
Рассмотрим исходное для S состояние. Они бивалентно. 
Значит в S есть и 0 и 1 валентные. Дальше если в S' есть 0 состояние или 1 состояние.Дальше см переходы из S. Провести эти рассуждения. 

![Brief scetch of last steps](img/20240928_092217.jpg)

И в S0 и в S1 могут быть бивалентные состояния. Говорим что S0 - переход в 0 по (h,m), S1 - в 1.
Всегда существует путь из S0->S1. Почему? Стартовое 0/1. Пусть этого пути нет. Пусть стартовое лежит в S0. Как то же оно должно попасть в S1. А вот он и путь. 

Если хосты разные отправляют разные сообщения, то порядок нам не важен. 
Значит h = h' Тогда все решает хост h. 
Крутимся через самое старое сообщение в сети.
Зачем самое строе сообщение из сети берем? Если будем брать не самое старое то некоторые сообщения будут лежать вечно. Из-за этого там могут лежать сообщения разных хостов. А мы переживам только 1 отказ(только сообщения 1го хоста могут лежать вечно). 

Мы ожидали 1 отказ. А мы построили 0 отказов и при этом имеет live lock. Где 1 отказ применяется?
Отказ нужен не для исполнения, он нужен что бы алгоритм **был бы к нему готов** и алгоритм выполняет какие-то шаги рассчитывая на то, что отказ возможен. 

Распределенные файловые системы. DFS

GFS - Google file system. Первая версия. Colossus.  

syscall. open, write,read,move,remove.
В обычной FS есть inode, хранящая информацию о файле, здеьс тоже будет иметь inode/
DataNodes где мы хранимы данные.\
Файлы будем разбивать на блоки - chunk. Что бы хранить в разных местах. 
Коэффициент реплекации - RF. В скольких нодах храним chunk. Начнем с RF=3

Replicator - дореплицирует недостающие chunk если количество записей меньше RF.
Chunk - Node ids, RF, 
Inode - Chunk ids, path, creation date, id. 

PrimaryMaster - RSM

Master-Slave replication
Chunk immutable в современных системах. И не фисированного размера. 
Хотим append в файл. PM находит nodes 

Merger.  Условно слишком маленькие chunk будет объединят в нормального размера.

![Scetch of GFS](img/20240928_095921.jpg)





## Лекции мфти 
### Лекция 1 

Очень разные типы распределенных системы:
- KV Storage 
- File System
- Coordination service ~atomic
- Message Queues
- Databases

Зачем нужны распределенные системы?
- Горизонтальная масштабируемость 
- Отказоустойчивость
- Проверка корректности данных, если не доверяем каким-то хостам.


Мы ждем от системы гарантий.
Пользователь, node- часть системы.

Message Passing. Внутри системы 
Shared memory. Снаружи системы.

Связь между node обозначим просто проводом. Почему?
Fair-loss link. Не гарантирует доставку. ->
Reliable link. Построим как tcp. по дублированию через id пакетов, по retry до получения Ac.
Вот здесь, мы считаем, что соединение или гарантирует доставку или порвалось. 

Время доставки сообщений:
Асинхронная - нет какого-то конкретного ограничения на время доставки.
- Safety свойство. Не делает плохо.

- Liveness свойство. Иногда делаем хорошее.
Частично-синхронная - существует точка t когда происходит переход safety- liveness.

Partition. Split brain. Система разбивается на две части. Да еще и по разному отвечает и работает.

Сбои node:
- Crash. Умирает и не восстанавливается.
- Restart. Перезагрузка узла 
- Византийские соглашения. Ведет себя хаатично.

Время. 
Надо считать timeout для failure detection а так же делать ordering. 

Для синхронизации часов лучше чем (t2 - t1) / 2 не сделать. То есть сеть работает симметрично. Т к можно доказать что алгоритма синхронизации не существует.

GPS в реальности синхронизирует еще и часы, трех пространственных координат мало. Навигационные  уравнения GPS.  

Google cloud Spanner. Реализуют TrueTime, где TT.now()  возвращает интервал \[e, l\], где настоящее время гарантированно лежит внутри этого интервала. TT.now() работает **ЛОКАЛЬНО**! Устроено это так, что сервера(Time master) синхронизируют время со спутниками GPS. А другие сервера(Armagedon master) содержат в себе атомные часы. И тогда происходит так: в момент времени t\* мы синхронизируется часы, получаем \[e, l\]. Когда через d нам приходит запрос мы возвращаем уже \[e + d \* const, l + d \* const\]. Где const - некоторое количество ppm, заведомо большее чем дрейф локальных часов. Но за счет частой синхронизации, интервал не расширяется на очень много.

### Лекция 2 

Рассмотрим KV Storage 
- Set(key, value)
- Get(key)

Репликация Register. Т.е. как хранить данные, помещающиеся в 1 машину надежно на разных.
Реализация: 
Конкурентная история : (то как в реальности приходили разные запросы к разным узлам системы)\
//////// w1
   ////////// w2
      /////// r
Последовательная история w1 w2 r w3 r. (Как мы пытаемся объяснить себе порядок выполнения)
У регистра есть несколько спецификаций - возможных последовательных историй.  

Модель согласованности - отвечает на вопрос "А какие конкурентные истории может порождать конкретная реализация для заданной спецификации?"

linearizability. ~External consistency
Для любой конкурентной истории существует последовательная история такая ,что, если в Конкурентной истории  одна операция O1 завершилась физически раньше другой O2, то и в последовательной будет сначала O1 потом O2. (Real time ordering)

Если система линеаризуема, то можно думать, что операции типо атомарные и выполняются как-бы **одна за одной**.
Это как раз про то, что операция выполнилась в какой-то определенный момент времени. До нее, операции не было, после она уже выполнена.

Atomic Register.
Single writer. 3 хоста.

Добавим к write timestamp, что бы понимать актуальность данных. Так ка writer 1, то timestamp он просто ++. Write(val, timestamp)
Write, отправляем запрос на все 3 хоста, ждем ответ от 2-х, пишем на 2. После полной записи на 2 хоста всегда отправляем AC клиенту.

Read.  Отправляем всем 3-м. Ждем ответа от 2-х. Получаем max(ts1, ts2).

Обобщаем 3 хоста до 2N + 1 хостов и вводим систему кворумов = N / 2 + 1.

Могут быть асинхронные кворумы, что бы одну операцию делать медленнее, а другую быстрее. Так, что бы увеличить чтение мы можем увеличить кворум на запись, но уменьшить на чтение.

Но если write еще пишет, а нам пришел read, то мы не знаем, про состояние write. Что бы следующий read прочитал как минимум то же что и мы, то после read соберем кворум на запись того, что прочитали.

В линеаризации выстроим все в порядке timestamp и он будет согласован с реальным временем. 

Multiple writer.
Надо выбирать timestamp распределенно монотонно. 
Фаза write:
Соберем кворум на чтение,  достанет max timestamp, возьмем себе timestamp + 1, соберем кворум на запись с нашим новым timestamp.
(может быть еще запись write в процессе, т е делать +1 может не сработать)
Commit Wait. timestamp можно выбрать не собирая кворум на чтение максимальной записи, а просто подождав некоторое время чего-то. 

### Лекция 3 
Общая задача - построить распределенный DataBase.
Есть Local Storage. Будем реплицировать(дублировать) данные на другие машины. Сверху на KV уже будет SQL.

Linearizability. 
Если в реальности одна операция выполнилась до начала другой, то и в линейном порядке их порядок будет в том же отношении.

Повторение про Multiple writer, single register.

Добавляем операцию CAS.
Проблема. Нужно хорошо понимать значение ячейки, и это должна понимать каждая реплика, а они не синхронизированы. Для того, что бы эти реплики сходилась к 1му значению мы используем timestamp. 
 
Когда мы просто читаем, пишем, то если нам приходит запрос с более старым timestamp,  его можно просто отбросить и забыть про него. А в CAS так нельзя. Значит нужен другой алгоритм.

 Хотим, что бы тот порядок, в котором команды приходят на реплику и был порядком их применения. Да еще и так, что бы этот порядок **у всех** реплик был одинаковым. Это реализует Atomic Broadcast.

Atomic Broadcast:

A-Broadcast(m). Отправляет всем сообщение m. Гарантирует, что рано или поздно на каждом узле, в том числе на исходном, вызовется обработчик.
A-Deliver(m). - Обработчик.

Свойства, которые мы требуем:
Validity - Если несбойный узел запускает Broadcast, то он вызовет обработчик на самом этом узле
Agreement - если какой то несбойный узел доставил сообщение, то и на остальных узлах это сообщение будет доставлено.
Total order - Общий порядок доставки для каждого узла. 

  Для любых двух узлов префиксы доставленных сообщений должны совпадать.
  --------x-----y
  ------x-----y---
  -------y crash node. Такое нельзя.

Рассмотрим State Machine. Автомат с состояниями, состояния переключаются.
Используя A-Broadcast() будем переключать состояние. Когда приедет на узел,  переключим состояние. 
Atomic broadcast как просто транспорт команд. 

Кворумы внутри реализации AB.

Линеаризуемость. Agreement + Total Order => такого быть не может.
Обработчик 2го сообщения на узле 1 должен быть вызван раньше чем обработчик 1го сообщения. Тогда сообщение должно было быть отправлено до момента его отправки 2м узлом.

Важное замечание - операция которая происходит в автомате, которую мы всем узлам даем должна быть детерминированной. (Хэш таблицы, время, дробные числа)

Еще одна проблема. Клиент может не получить AC так как нода умерла. По timeout мы его поймали. Retry мы сделать не можем, так как клиент не знает состояния системы, применилась ли команда или нет? Тогда может вознинуть повторение действия команды. 
Семантика Exactly ones. 

RSM много. Внутри есть Total order. Но снаружи, на всю сеть RSM этого total order нет.

#### **Реализация** Atomic Broadcast.

^fd9023

1. Сообщения нужно доставить
	Reliable broadcast.
		Гарантирует:
		Все сообщения, которые были отправлены корректными узлами будут получены всеми корректными узлами. и все получат один и тот же набор сообщений.
	Отправляем всем свое сообщение. Когда какой-то узел получает первый раз новое сообщение он его отправляет всем. 
	Все. Это и позволяет соблюсти гарантии.


2. Сохрнить порядок сообщений.
	Consensus.
	На каждом узле нам нужен алгоритм propose. На вход дается input. Узлы все вызывают Propose(input). На выход получают некоторый общий out. Этот out должен быть: 
	Из предложенных значений(Validity),
	 Единый для всех узлов. (Agreement),
	 Алгоритм должен завершаться (Temptation).
	
	Будем делать серию консенсусов. 
	A-Broadcast =R-Broadcast
	A-Deliver(m) = R[] += m (Добавим сообщение во множество сообщений, доставленных RB). Для них обработчики доставки еще не вызваны.
	A[] - множество сообщений, для которых обработчики доставки уже были вызваны.
	Фоновый процесс:
		Раунды. r = 0
		while R[] - A[] != 0 (пока есть сообщения, которые еще не были доставлены )
		r = r + 1
		Bi[] = Propose(r, B[]) Предлагаем на консенсус те сообщения, что были нами получены но еще не отправлены R[] - A[].
		На всех сообщениях из Bi[] вызываем обработчик доставки 
		A[] += Bi

	Все. В каждом раунде доставляем какое-то количество сообщений. Все зависит от Propose.

Как решить consensus с помощью AB?
Пусть каждый узел в propose выполнит AB и выберет то, что вернет ему первый обработчик доставки. 
Но мы хотим наоборот AB через consensus и reliable broadcast.


Итого 
RSM > AB > Consensus.
RSM - хорошо, сможем мысли о системе в которой несбойные надежные узлы , но нужно научиться делать Propose.





### Семинар 1
UTC - добавляем високосные секунды, что бы время совпадало с наблюдаемыми физическими явлениями.
GPS - в начале добавили немного високосных секунд, что бы синхронизовать с UTC с 1980х.
TAI - атомные, високосных секунд нет, меряем только по физическим процессам в атоме цезия.

Wall time  - Общая точка отсчета для всех node, но ведут себя не монотонно. Это как раз то, что не возможно синхронизировать
Monotonic clock. Разная точка отсчета для всех node, но ведут себя монотонно.
В c++ system_clock - wall clock
steady_clock - monotonic_clock

Leap smear - исправление високосной секунды. 
То есть секунды в компьютере будут идти чуть чуть медленне, что бы убрать эту разницу. 

Single point of failure. Единая точка отказа. Делаем систему так, что бы вот этой точки отказа не было. Что бы потеря какого-то одного оборудования не обрывала бы работу всей системы.

Много маленьких кластеров - не выгодно. Делаем один гигfнnский кластер, что бы было много связей.

Даже у TCP есть проблемы. У client мы не видим полной картины. Сервер может перезагрузиться, выключиться, а tcp подключение на client'е может остаться. Это проблема, это не чиниться, это надо учитывать.
Мб [тут](https://gitlab.com/users/Lipovsky/projects) можно поискать задачи/реализацию.

