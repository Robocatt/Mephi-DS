### Книги по рекомендации Бидзили
Introduction to Reliable and Secure Distributed Programming - теория
Designing Data-Intensive Applications The Big Ideas Behind Reliable, Scalable and Maintainable Systems - практика
[Плейлист](https://www.youtube.com/playlist?list=PL4_hYwCyhAvaYKF6HkyCximCvlExxxnrC) Романа Липовского для мфти
## Лекции в МИФИ
### 04.09.24 Лекция 1
Иерархичность серверов => Главные меняются. Clos архитектура.
Система хостов полносвязана, имеет локальные диски. Общих данных нет, только обмен сообщениями.
Клиент как именно часть распред системы.
- Отказоустойчивость
- Не хватает одного хоста (вычисления/память)

Задача "консенсуса" как самая сложная в распределенных системах.

Задача: (похожа на  nosql )
key-val storage 
set(k, v); get(k) -> v.
Подразумевает асинхронность, но ответ даже на set необходим обязательно.
Лучше не отвечать, пока гарантированно не записал, у себя и желательно в остальных хостах.

Игнорим ошибки памяти(византийские гарантии),cpu, шанс что диск записал не то (check sum).
Host: 
- crash free;
- crash stop - сервер не реснется; 
- crash recovery может восстанавливаться и умирать постоянно. или полностью умереть как в crash stop.

Сеть - пакеты:  пусть нет corrupt, но может быть drop, а также duplicate. 

Fair loss  - рано или поздно сообщение будет принято

Perfect link - идеальная модель в которой все доходит 1 раз. 
Но даже в ней с умирающими хостами приколы.

Fairloss + tcp/ip = perfect link в достаточных для практики  пределах
Работаем с perfect link но не смотря на это помним про проблемы 

1.  Отказоустойчивость: не смотря на смерть хостов данные будут сохранены  
2. Масштабируемость (вплоть до линейной)
3. Консистентность

~40 шт хостов - стойка. Обмен данными к хостам через общий switch.
Тогда важно писать в хосты на разных стойках.
Иерархия доменов отказа.

round trip time 1-3 ms в data center 

Два вида свойств/гарантий в распределенной системе:
1. Safety Система ведет себя или корректно или нет.
2. Liveness . Три режима ответа сети. О времени ответа сети,  том, когда будет полезный ответ. Асинхронная, синхронная, частичная. Частичную рассматриваем. 

Время и часы.
Дрейф часов. Кварцевые ~1.7 sec/day. Атомные. Можно сказать не дрейфуют.
Но синхронизация часов тоже требует время.
Физические часы:
wall clock - аппаратные. Синхронизируются, есть високосные секунды.
monotonic clock - Локально. С момента старта. Никогда не откатываются.
Логические часы.

не можем синхронизировать больше чем eps.

### 07.09.24 Семинар 1
Сем. Ссылки с него 
Некий [userver](https://userver.tech/)
Описание про [thread](https://wiki.osdev.org/Thread)
[Примеры](https://github.com/chriskohlhoff/asio/tree/master/asio/src/examples) асинхронности, синхронности сервера



### 11.09.24 Лекция 2
K-v storage.
Regular Register.
Write (val)
Read->val

Single write. 
3  хоста всего. Можно ли писать на 3 хоста? На 1 нельзя, так как он сам может умереть. На три нельзя, потому что хосты умирают и если будем писать на 3 при смерти хоста запись не будет выполнена. Пусть пишем хотя бы на 2 хоста всегда. 

Надо выбрать все три  и дождаться ответ от двух.

Читать с 1го хоста? Нет, может умереть.  Или может сохранять старое значение. 
Читать с 3-х хостов, нет так как если при такой системе умрет один алгоритм никогда не завершится.
Читаем с 2-х хостов. 

Соответственно пишем/читаем на 2 хоста.  Тогда если читаем, то обязательно пересечемся хотя бы с 1-м из тех хостов на который писали. Кворум. Читаем кворум, пишем кворум. 
Как определить где новые данные? **Timestamp**.Их генерит Single writer. Именно single, что бы можно было просто ++ на счетчик и ничего не синхронизировать. Логическое время.
Что-то типа requestId не работает. Так как они не упорядочены. Упорядочевать id в распределенных системах - задача сложная. 


В целом думают вначале так, что у нас есть лидер, потом решают как от этого избавиться т.е. как выбирать лидера.
| w1  |   |                         w2          |  
   |     r2   |
          |    r1            |
Такое возможно. Так как для  read2 может быть кворум:
 |         /    |  
 |2    /1/    |  
 |    |  | | |  | |   
|        / 1  / |  
     /

Write majority.
N хостов. 
2N -> N+1 пишем .  Fault tollerance  N - 1
2N +1 ->N+1 пишем. Fault tollerance N
Fault tollerance  - максимальное количество отказавших хостов. 

Кворум- любая система множеств в которой любые 2 множества пересекаются. 

Ассиметричный кворум - пишем на 4, читаем с 2-х. Типо чтение - чтение могут не пересечься. Используется когда очень много чтений.

Четное число хостов не уважают и считают что так не делают.
Так как кворум одинаковый, а Ft меньше.

Модели консистентности. 
Самая "слабая":
Eventual consistency  - когда нибудь все будет отлично и система придет к консистентному состоянию.

Самое "сильное" свойство.
Linearizability
Linearization point.
Операция произошла в какой то **определенный** момент времени. До него операции не было, после точки она есть.

Sequential consistency.
Разложение истории на временную ось без пересечений. 


Atomic register. 
Read Impose 
|read from quorum| impose |
impose = write  on quorum



### 14.09.24 Семинар 2
Процесс - единица исполнения, обладающая ресурсами(памятью)
Поток - -//- не имеет своей памяти 
На стороне OS и управляет OS через sheduller с вытесняющей многозадачностью.
FIber - кооперативная многозадачность. Работает пока сам не остановит себя.
Subroutine - просто функция  в ЯП
Coroutine - функция, что может быть остановлена и потом быть восстановлена с того момента, на котором была остановлена.

Stackfull / Stackless 

Callback генерятся сами внутри co_await 
В stackfull нет этих слов, там будет прям объект coroutine и его pure методы.

Примитивы синхронизации для fiber будут немного отличаться, ведь надо стопить определенный fiber, а не весь thread.

Courutine - при использовании co_wait, сама вставляет callback.

[Фреймворки](https://www.techempower.com/benchmarks/#hw=ph&test=fortune&section=data-r22) с fiber.

Userver
Почиать [документацию](https://userver.tech/)


### 18.09.24 Лекция 3
1 Writer -> multiple writers. 
Было у каждого write некоторый строго увеличивающийся timestamp. Порядок строился на них.
R  = R + W
W = R 
Собрали кворум на R из этого read получили timestamp потом делаем W (timestamp + 1)

Пришло два w по одному timestamp. Надо линеаризовать => упорядочить, но не важно как. Тогда добавим к timestamp у W  какой-то глоабльный уникальный ключ - GUID. И все, посортим по нему, ведь порядок должен быть любой. 

Доказательство линеаризуемости. 
    01
R + w         O2
          R + w
Кворум на O1 W и  O2 R пересекутся, получим из них timestamp. 
timestamp O1<=timestamp O2


Операция CAS. Compare and swap
```
CAS(k, v, v*)
	if (stor[k] == v){
	stor[k]=v*;
}
```

На какждом хосте должен быть один и тот же порядок что бы CAS работал корректно.
(Тогда CAS должен быть блокирующим для машины?)

Replicated State Machine.
	A replicated state machine is **a deterministic state machine where multiple machines have the same state and run the same**.
({stat}, {transition}). Transition - какой-то переход  внутри хоста по автомату.  
Хотим автомат разложить на все реплики в начальном состоянии? 
Хотим что б ы на каждом хосте операции прмиенились в одном порядке, что бы в конце концов все хосты были в одном и том же состояниию.
?????

Atomic Broadcast
A-Broadcast(m) <- отправить всем сообщение 
A-Deliver(m) <- обработчик
Свойства ^7c1e2a
- Validity, если не сбойный узел сделал  A-Broadcast(m) то он позовет A-delivery.
	Узел сбойный тогда, когда он умирает и не взаимодействует больше с другими узлами. Все остальные узлы не сбойные. 
- Agreement , если на не сбойном хосте позвался delivery(m) , то на всех остальных он тоже позавется. 
Получаем гарантию что все сообщения будут получены несбойными хостами. 
- **Total order**, Для двух хостов посмотрим их историю в любой момент времени. Тогда один из них является префиксом другого или наоборот.  

RSM с помощью Atomic Broadcast.
Broadcast оставляем.
Deliver - переключить состояние. 
При запросе: Broadacst, ждем deliver, при получении отправляем ответ клиенту.

Reg < Reg + CAS < RSM < AB < Consensus


Linearizability AB

То есть по total order другого порядка нигде быть не может, а значит и линеаризуемо.

Если узел координатор(к которому в начале пришел клиент) умер до отправки ответа клиенту, а такое может быть. То, это уже должен фиксить клиент. То есть если он не получил ответ, то он должен еще раз сходить в систему и сделать check или retry.// Exactly once нигде нет. 

Mutex = RSM 
Node &harr; Node

RSM умеет переживать перезапуски машин!

Propose(v) -> v\* 
v\* вернется на все хосты, причем $v* \in {v1,v2,v3}$

Validity - safety (на конечном промежутке)
Agreement - safety (на конечном промежутке)
Termination - Liveness ( на бесконечности)

AB:
Validity - liveness
Agreement - liveness
Total order - safety  


AB -> Consensus
Ab(m1), delivery вернет то, что типо должен вернуть propose
Ab(m2)
Ab(m3)

Consensus -> AB
Reliable Broadcast

Если сообщение пришло на хост первый раз то еще раз отсылаем всем и потом отправляем обработчик не дожидаясь ответа. 
AB: 
R-B(m) + R-D(m): Buf += m

Background: постоянно крутится процесс. 
Несколько раундов консенсуса. A = отправленные сообщения 
while buffer - A not empty{ 
propose(всех сообщений из buffer - A), вернет R\*
Ad(r\*)
}

### Лекция 4

~Кривое доказательство что cas не работает без state machine.

Алгоритм  работы AB. Смотри [[Распределенные системы#^fd9023]]

Задача бинарного консенсуса. $v* \in {0,1}$

Мы не отличаем partition от отказа узлов. 
При разделении посчитаем, что меньшая часть не будет выполняться. 
Тогда любой алгоритм консенсуса переживает $\frac{N - 1}{2}$ отказов.

[Теорема FLP.](https://neerc.ifmo.ru/wiki/index.php?title=%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%A4%D0%B8%D1%88%D0%B5%D1%80%D0%B0-%D0%9B%D0%B8%D0%BD%D1%87%D0%B0-%D0%9F%D0%B0%D1%82%D0%B5%D1%80%D1%81%D0%BE%D0%BD%D0%B0_(FLP))
Опр. Для любого алгоритма консенсуа который переживает хотя бы 1 отказ существует такое исполнение что алгоритм не завершится. 

Говорим что никогда не нарушаем Validity и Agreement. Значит можем нарушать Termination.
Доказательство. 

Конфигурация - состояние всех узлов + все сообщения в сети. Типо Snapshot. 
Есть направленные переходы между узлами: Доставили сообщение m.
Получили так граф конфигураций.

Исполнение - путь в графе = порядок доставки сообщений. 

Графом конфигурации описывается любой алгоритм консенсуа. Конкретное исполнение - конкретный путь. 

Бесконечное исполнение - бесконечный путь ИЛИ цикл.

Доказательство итеративное.

Лемма 1. ~Существует начальное состояние нам нужное.  = начальное бивалентное состояние. 
Для бинарного консенсуса. Каждая нода позвола propose.  Получается  $2^{n}$ начальных конфигураций.
Валентность конфигурации - то что вернет консенсус.
Конфигурация унивалентная есть мы знаем с каким значением завершится алгоритм. 
Конфигурация бивалентная если не знаем.

- 0 валентная

$$
R_{0} = \begin{pmatrix}
0 \\
\dots \\
0\end{pmatrix}
$$ 

- 1 валентная

$$
R_{n} = \begin{pmatrix}
1 \\
\dots \\
1\end{pmatrix}
$$ 

$$\Rightarrow \exists k : R_{k} \text{0 валетная, } R_{k + 1} \text{1 валетная,} $$
Они отличаются только значением k+1 хоста. Так как алгоритм переживает паделние 1 узла, то этот k+1 узел можно выкинуть. А значит оба этих узла бивалентные. 


Лемма 2. ~Всегда существует переход, который приводит в не завершение. = для любой бивалентной конфигурации можно найти следующую за ней бивалентную.
$$\forall (p,m), \forall c \text{ бивалентное }\exists \text{ путь } c \to c' \text{ что (p,m) , будет доставлен последним в этом пути и  }c' \text{ бивалентное }$$

Причем в этом пути не должно быть более 1 хоста умерло. 
S - множество конфигураций где (p, m) не доставлено
S' доставлено из S (p, m) последним.
А за границей этого контура что-то что после s'.
![S Sets](img/20240925_094206.jpg)

Лемма 2.1. Хотим доказать что в S' есть бивалентное состояние.
Докажем от противного. Пусть там только унивалентные. 
Рассмотрим случаи где может быть 1 валентная, 0 валентная????
Значит в s' есть и 0 и 1 валентная.

1 в s => если из 1 валентное доставляем сообщение, она остается 1 вал в s'
Если за границей то туда мы могли прийти из s' по .


Лемма 2.2.
Найдется такая конфигурация что в ней решится судьба вернется 0 или 1. Доказательство. 


### Семинар 3
Теорема. (повторение  из лекции).
faults < n/2
Не различаем partition и падение узлов.
Слева 3 node (A) справа 2 node (B)
При потере сети Nodes справа буду тупо бесконечно ждать. 
То есть хоть в какой то части должно быть нарушено свойство termination.
faults < b или faults < a <=> faults < max(A,B)
minimize(max(A,B)) -> A~B~n/2. 
n = 2k+1  A = k +1 b = k
n = 2k a = b=k
f < n / 2.

Алгоритм консенсуса не может завершиться даже если f >=n/2.

FLP теорема (CAP теорема consistency, availability, partitioning - нельзя построить распределенную систему, в которой выполняются все 3 свойства.)

FLP на практике бесполезна, а теорема о n/2 на практике учитывают.
Так как в FLP говорится, что если случился 1 отказ то сценарий как там теоретически может случиться. И по факту такое бывает редко 

Dead lock и Live lock - программа что-то делает вычисляет, но по факту полезная работа не выполняется.
При нарушении FLP будет livelock, при нарушении n/2 в системе все вообще плохо.

FLP повторение доказательства. 

У каждой node: in memory per host, disk per host, outgoing message. Назовем это конфигцрацией.
Строим граф конфигурация. Исходящих ребер максимум количество сообщений в сети. 

 
Рассмотрим исходное для S состояние. Они бивалентно. 
Значит в S есть и 0 и 1 валентные. Дальше если в S' есть 0 состояние или 1 состояние.Дальше см переходы из S. Провести эти рассуждения. 

![Brief scetch of last steps](img/20240928_092217.jpg)

И в S0 и в S1 могут быть бивалентные состояния. Говорим что S0 - переход в 0 по (h,m), S1 - в 1.
Всегда существует путь из S0->S1. Почему? Стартовое 0/1. Пусть этого пути нет. Пусть стартовое лежит в S0. Как то же оно должно попасть в S1. А вот он и путь. 

Если хосты разные отправляют разные сообщения, то порядок нам не важен. 
Значит h = h' Тогда все решает хост h. 
Крутимся через самое старое сообщение в сети.
Зачем самое строе сообщение из сети берем? Если будем брать не самое старое то некоторые сообщения будут лежать вечно. Из-за этого там могут лежать сообщения разных хостов. А мы переживам только 1 отказ(только сообщения 1го хоста могут лежать вечно). 

Мы ожидали 1 отказ. А мы построили 0 отказов и при этом имеет live lock. Где 1 отказ применяется?
Отказ нужен не для исполнения, он нужен что бы алгоритм **был бы к нему готов** и алгоритм выполняет какие-то шаги рассчитывая на то, что отказ возможен. 

Распределенные файловые системы. DFS

GFS - Google file system. Первая версия. Colossus.  

syscall. open, write,read,move,remove.
В обычной FS есть inode, хранящая информацию о файле, здесь тоже будем иметь inode/
DataNodes где мы хранимы данные.
Файлы будем разбивать на блоки - chunk. Что бы хранить в разных местах. 
Коэффициент реплекации - RF. В скольких нодах храним chunk. Начнем с RF=3
Пишем на 2 ноды, на 3-ю дореплицируем потом репикатором. 
Replicator - дореплицирует недостающие chunk если количество записей меньше RF.
Chunk - Node ids, RF, 
Inode - Chunk ids, path, creation date, id. 

PrimaryMaster - RSM

Master-Slave replication
Chunk immutable в современных системах. И не фисированного размера. 
Хотим append в файл. PM находит nodes 

Merger.  Условно слишком маленькие chunk будет объединят в нормального размера.

![Scetch of GFS](img/20240928_095921.jpg)

### Лекция 5
Продолжение семинара.
Есть много data nodes.
Есть file, состоящий из chunk.
Хотим в файл уметь append, read,move,open,remove.
Есть  еще pm(rsm внутри).
```
Open = pm : ReplicatedOpen(request)
	if file  does not exist
		create file
	return descriptor
```

Append
1. client->Pm.open(),
В Pm не пишем данные, будет слишком нагружаться.
1) build chunks
2) store chunks
3) update chunk ids(in file)
Разделение на chunks можно встроить в клиентскую библиотеку. Не лучший выбор, так как при желании обновления chunk придется обновлять всех client, но можно начать с этого.
Лучший способ - использовать некоторый proxy, где будет наш код который мы всегда сможем обновить. Ходить к Pm/  data nodes будем через proxy. (Пока не рассматриваем.)

Заведем SecondaryMaster(SM тоже RSM внутри) которые будут отвечать за chunks и data nodes.(Какой chunk лежит на каких nodes) (PM знает про дерево файловой системы).

2. client -> Sm  creat chunk
3.  client->data nodes store chunks
4. client->pm update chunk ids

Есть две стратегии информации о chunks в SM. Вторая  : data nodes хранят инфу про то,какие они chunks хранят и периодически отсылают  эту инфу в SM. 

Итак, в SM храним struct chunk,в котором RF, chunk id, node ids.
```
Replicated Append(request)
	File chunkids.append(request.chunkids)
```
Нужно еще учесть:
После restart проверять валидность данных.


Datanodes <->SM посылают раз в n секунд heartbeat.
SM:
Transient state(in memory) : chunk, node -> last alive time
Persistent state(RSM): chunk->node ids, rf
Heartbeat обновлять будет Transient state.

Current rf можно вычислить по списку node в transient state.

Шардирование. Данные, разбитые на маленькие блоки назовем shards. Нужно что бы не искать в 1 м большом чем то, а разбить на несколько и искать в нескольких.
В целом, SM может быть шаридированным. (PM не может быть. Не понятно как шардировать дерево).

Шардирование и heartbeat - то почему нужны SM. PM  бы не вывез.

Replicator. Как делать репликацию. Делать надо в SM. 
1. Under replicated chunks
2. Pick chunks for replicator
3. Pick alive node <- source target node
4. Send replicate request to source node 

Может ли быть 2 replicate? Если юзать чисто transient и узлы падают, то да.
Overreplicated chunk тоже бывает, от них стоит избавляться тоже.

Диапазон между retry.
Jitter(случайная величина) и backoff(постоянная) определяют это время ожидания.

Консистентность append. 
Вариант 1 При открытии брать lock .На момент записи проверять есть ли lock.
Вариант 2 Механика prerequisite timestamp. Каждое действие PM будет снабжаться  timestamp. 
В File будем хранить Modification  timestamp 

Read return file mts (FMTS)
при append будем слать (FMTS). Если такой же, то файл не меняли в промежутке. А если нет, то кто то менял и чанки не валидны и надо заново делать append. "оптимистичная блокировка"

Делать retry read, так как обычно  write меньше и вклиниваний будет  мало. 

Move происходит строго в памяти PM. Так и все операции над директорией происходят чисто в PM.


### Семинар 4

gRPC
Protobuf

### Лекция 6 
Повторение DataNode,primary master. 

Map Reduce. 

Данные хранятся в виде таблицы 
table = list of rows. Row = map\[name_col\] -> value.
table = list (key,value). 
Просто 2 способа говорить об одном и том же 

Операция map(row) -> row\*
map(key, value)->\[key\',value\' \]

Операция reduce(key,\[value\]) -> \[value\]

Пример. Посчитать количество слов в таблице. 
Утверждение. Практически все вычисления можно делать в терминах map/reduce.

ps -efl | awk '{print $3}' | sort | uniq -C
		map                       reduce

Inner join. Декартово произведение пересечения двух множеств. Брем 2 входа и объединяем по ключу. 

Реализуем Hadoop v1.
Параллелим map. Разбиваем просто input на части. Передают машины все в shuffle + sort облако.
1. Приходит клиент с job = \[tasks\], input, output, mapper, reducer, num of map nodes(M), num of reduce nodes(R). Task = map/reduce. Приходит он в некоторый jobTracker.
2. jobTracker идет к PM за дескриптором в DFS
3. Дробит input на M частей и отправляет на map nodes. Сама выбирает какие то nodes  куда отправлять. Сама поддерживает связь через ping с этими map nodes.
4. Map. Накапливает результат в R штук buffer, которые лежат в памяти map node. Partition - buffer для конкретного reducer. Как понять в каком partiotion?  hash(key) % R. Если buffer большой кидаем на диск. Но перед этим вызываем на нем sort.(external algorithms)
5. Хотим на каждую partition теперь 1 файл. Сделаем merge по partition merge sort. Получаем ровно R файлов в ЛОКАЛЬНОЙ системе map node которые внутри отсортированы.  
6.  Shuffle
7. Sort
8. Map после завершения говорит jobtracker результат операции map, когда тот завершился. 
9. Так, Reduce node вычитывает partition из той map node, которая это считала. Reduce node получает это знание из того что ему jobtracker послал. Jobtracker знает, так как map по завершении ping об этом.
10. Результат reducer записывет в DFS. (будет R файлов) 

+:
map, reduce распараллелены. 
Используют HDFS. input имеет некий RF. Так как файл уже лежит на многих машинах попробуем запустить map на тех машинах, где файл уже лежит. Таким образом оптимизируем сеть. 
Locality (оптимайз сети)
Combine (оптимайз сети) 

Fault tolerance. Если падает map или reduce переделываем только task.  Но стоит делать limit. А то так можно получить бесконечный цикл, поскольку пользователь может дать некорректный код. 

Фикс падения job tracker - иногда скидываем в  локальную память(а можно и в DFS) state of tasks. 

Speculatie execution. (+time - cpu) Посылаем на несколько node одну и ту же задачу. Выгодно, так как некоторые nodes быстрее некоторые медленнее.  

Записи в локальную FS : 1 сброс buffer + merge + reducer merge. При записи на DFS можно сделать лишь 1 запись. Объединение файлов через указатели.

R < M. 
DFS А также HDD лучше писать больште куски. Пожтому R лучше не делать маленьким. 
Ограничений на M особо нет. 

Hadoop v3 YARN
Решили для 1 го клиента = 1 job. Как решить для нескольких клиентов = несколько job?
Введем AppManager, который следит за jobs. 1 job = 1 appmanager. Идет к resource manager и спрашивает какие узлы доступны. Resource manager следит за тем, какие node работают.  



### Семинар 5




### Лекция 7 
Raft consensus


### Лекция 8. Локальное устройство DB.
Кр: задизайнить что-то плана отказоустойчивого сократителя ссылок. (книжный пример, будет что то иное)

Odered K-v storage persistent local. Отказоустойивость на 1-й машине.
- put(key, val)
- del(key)
- get(key)
- scan(low, high) -> iterator (Получить все ключи в этом интервале)
Хотим параллельность всех операций.
- Операция Snapshot() -> k, v storage read only. Get и Scan переходят сюда.
Persistent = переживаем перезапуск, хранить будем на диске.
В памяти можно сделать std::map / другие деревья.
В map мы обходим дерево, если память, то это доступ в памяти и это ок. А вот когда мы захотим перенести в диск, то придется постоянно читать с диска, что как известно не здорово. Тогда одно обращение ~2-10мс для HDD. 
Выгодно писать последовательно много. 

SSD.  Набор блоков, блоки разбиты на страницы. Построен на флэш памяти. FTL.
read(page)
program(page) - set 1's to 0's. Только применимо  к чистой (состоящей только из 1)  странице.
erase(block) - set all block to 1. 

Пользователь записал какую-то страницу. Хотим ее изменить -копируем страницу в новое место и меняем на ходу. Страницы ~4Kb. В фоне garbagecollect которые отслеживает состояние блоков и если надо делает erase. 
SSD выгодно что бы писали что-то новое, что бы не перезаписывать данные. Выгодно что бы писали последовательно - пишем целый блок. 

Для того, что бы map работало в памяти, хотим писать последовательно много и что-бы записи были новые. 


B - дерево. В node храним пачку значний K -v. и  B nod на уровне. Выгодно, потому что высота сильно меньше. То есть когда будем обходить дерево, меньше чтений получим. 
B+ дерево. Храним только ключи в node. Значения в листьях. Все node соединяем в список. 

Быстрый Get.
Разобьем сортированный файл на блоки и у некоторого index будет key, offset. Табличку index хранится и на диске и в памяти.  Бинпоиск по ключам (в памяти). нашли offset, вычтем начиная с offset данные. И уже там найдем ключ.  Оптимизация чтения с диска с log раз до 1 го. В index key, offset. Так как мы знаем offset то unzip этой части. 

Быстрый Put (k, v).
Храним некий Log - очередь. 
в Log просто кладем всю операцию (put(k,v)).

Тогда get будет искать первую запись с конца. Будем хранить в памяти MemTable, который будет запоминать последний put в этот ключ. Тогда get ходит только в memtable, put пишет и в log и в memtable.
Когда memtable большой скинем его в файл на диск, как мы делали в get. memtable -> dump -> sstable. sstable - тот сортированный файл и таблицца с индексами. sstable будет несколько. К тому же можем еще и zip делать. Чтение будет всего одного диска, по этому 1 unzip ничего. Теперь можем и log удалить, так как все это в sstable. Файлы sstable связаны между собой. 

Static to dynamic. Файлы размера степени двойки, делаем merge когда у 2-х файлов совпадает размер. Compaction.

Delete (k) = put(k, flag). 

Получаем 1 запись = 1 чтение с памятью.  

LSM log-structures merge-tree.

Фильтр Блума. Храним в памяти. 
set[]
- contains(k) -> No\Maybe 
- put(k)
Будем иметь несколько хэш-функций {h}. 
Put(k):
h1(k), hn(k) - позиции в массиве. На эти позиции ставим 1. 
Contains:
h1(k),hn(k) - Если попали в хотя бы в один 0, то точно знаем что нет элемента.
tradeoff error vs size

Тогда sstable = sorted file + index + filter и на диске. index + filter и в памяти тоже. 
При поиске ключа в sstable вначале проверим в sstable.  


Как устроен Memtable. 
Skiplist. Отсортированный list с некоторыми еще указателями вперед.
С какой то вероятностью продолжим добавление на уровень вверх. 
На put и get будет o(log ) как в treap. 

Реализация LSM - levelDB google, rocksDB. 

Всегда новая запись будет в такой реализации.  Только append. Значит можно это делать и в DFS.  

Snapshot нужен для get и scan. Он использует  memtable. Можем просто делать персистентый skiplist. Или же на каждый put делать  fork, так как там COW. 




raft, lsm, flp, mapreduce

### Лекция 9.  KV DB, шардирование, шард = RSM

#### Raft. Доказательство последней теоремы. 
Committed entry:
1. Записано на кворум в этом терме
2. Или есть закомиченная после.
2 является следствием logmatching Property и 1.
Доказательство, что такие записи действительно закоммиченные. 
~Если запись закоммичена, то она будет в логе любого лидера в дальнейшем. 
От обратного.
База. Нет предыдущих лидеров.
Индукция.  Выберем первую ноду, в которой нет интересующей нас записи. 

Рассмотрим первый лидер в логе которого нет записи.
Как мы его избрали? Рассмотрим предыдущий терм. Там должна быть та запись, которой нет в лидере, по предположению индукции(для всех предыдущих лидеров есть запись) она там есть. А удалиться она не могла, так как лидер не удаляет свой лог никогда.

Где применили то, что кворум в текущем терме?
1. last log term = x. По log matching poperty ничего другого записать не можем.
2. last log term  > x 
Индуктивное предположение применимо только после сбора кворума.


#### Apply в RSM
RSM = State machine + Log + persistent state.
State machine всегда можно восстановить из лога.
Response to client идет из state machine.

Когда из Log делать apply в state machine?
 Если last commited entry = 10 в текущем терме была закоммичена, то можно применять в RSM записи от предыдущей до 10. Last commited entry храним на диске.  
 persistent state = LCE, 

#### KV Database
YT dynamic table/YDB. Spanner/HBase/DigTable.

Static table на многих хостах.
Множество чанков в цепочке. write/read. Записали что то большое, позвали map reduce.

Dynamic tables.
latency~10-100ms
Throughput ~ 10\^7 rps.
Key count >= 10\^9
size >= 10\^13 byte.

Сущность. Tablet. Таблица разбита на Tablets, которые отвечают за диапазон ключей.
Одна из колонок key по которой все отсортировано. 
Шардирование - деление всего: памяти, compute, сеть, состояния. Позволяет одновременно писать в x раз быстрее. 
Chunks - деление только памяти и досупа к таблице.
Размер tablet минимизируют так, что бы overhead был минимальным. ~100MB.

Всегда количество таблетов следует немного увеличить, позволяет распределять нагрузку лучше. Так же позволяет не ждать очень много, если tablet падает. 
На хосте может быть несколько tablet. 
Tablet = RSM.
Primary Master. Храним инфу о таблетах о хостах. Так же структуру что бы делать запросы по шардам. 
PM отсылает heartbeat нодам. Хосты тогда меняем когда некоторые падают.
Производит балансировку нагрузки и решает задачу планирования. Планировщик в отдельном треде. В многопотоке определяем что делать, потом отслыаем в single thread.

Put/get идут в pm узнают tablet опять идут в pm и получают данные. Как исправить?
Кэширем инфу с PM. Меняется инфа только когда хосты умирают. Это не так часто. Так что меняться надо будет только через много 10\^7 запросов.


Гарантии консистентности. RSM дает линеаризуемость. Линеаризуема в терминах 1-й простой операции. Put 10 ключей линеаризуемым и даже атомарным не будет. Проблема из-за доступа к шардам разным. 

RSM base = term, log, last entry , ..., last commited
State machine k = a, v = x; k  = b, v = y;
Деление этой таблицы в памяти - LSM.
Когда запись в LSM когда в state machine. 

Таблет - консенсус группа 3(n) хостов.





### Семинар 9. Транзакции
#### Транзакции

2 Клиенты хотят оба write(x), write(y) 
Может быть как угодно, а хотим что бы или write1(x) и write1(y) прошло полностью или только у 2 клиента. То есть транзакции возникают, когда несколько ключей берем. Не проблема когда в 1м шарде(RSM), а если в разных, то проблема, порядок в шардах разный. 
Mutex - та же самая транзакция. У транзакции такая же семантика. lock(), code, unlock().

Транзакции тогда когда клиент обращается к нескольким ключам и это обращение было упорядоченным, с нормальными гарантиями (консистентным).

API Transactions .
startT
abortT
commitT

Теперь клиент не ходит в KV, а в scheduler со startT, потом Read, write. Scheduler ходит в KV.
abortT если KV вернуло к примеру не то, что ожидали. 
commitT фиксирует все в транзакции.

Гарантии транзакций - ACID 
Atomicity,Consistency, isolation, durability.

I - Две транзакции клиентов независимы
D - если случился коммит, то все операции транзакции применились и уже никогда не отменятся.
A - Все операции транзакции или применятся целиком 1 раз или не применятся вообще. 
C - Транзакция оставляет KV в консистентном состоянии. (После транзакции сохраняются инварианты пользователя.) По идее, AID уже дает consistency. 
#### Scheduler устройство 

Вспомним, что KV линеаризуемо. 

Scheduler -> \{schedule } (упорядочивает операции)

Расписание - какая-то последовательность операций в KV.
Серийное расписание - все операции 1й транзакции не перемешаны.
Реализация - просто mutex. Берем клиента, остальных блочим.

View equivalency - два расписание эквивалентны если на каждый read(когда ключи одинаковые, когда разные, то вообще без разницы) они вернули то же самое, но после всех операций оставили KV в одном и том же значении.

Расписание S View serializeable - когда существует такое серийное расписание S*, что S ~(view equivalent) S* .

Фича в том, что клиент думает что порядок операций строго T_x, T_y, T_z, в то время как фактически они перемешаны. 

Примеры таких расписаний 

Можем ли мы сделать алгоритм, который пораждает view serializable расписание. 
Получим другой класс, более строгий, который можем описать строго.

Strict serializable = view-serializable + realtime order. Если транзакции  упорядочены во времени, то они упорядочены и в серийном расписании. 

Конфликт KV - две операции если, они принадлежат разным транзакциям,  они обращаются к одному ключу, и хотя бы одна из них на write.

Два расписания конфликтно эквивалентны (conflict equivalent) тогда когда они одинаковы в точности до перестановки  неконфликтных операций.

Конфликтная сериализуемость (conflict serializeable) - расписание S когда существует  серийное расписание S* которое ему конфликтно эквивалентно.

Конфликтно сериализуемое расписание удобно представлять в виде графа конфликтов.
Вершины - транзакции. 
Ребра = Дуга(ориентированное ребро): $$ {(T_i, T_j) }, \exists O_x, O_y \&  O_x конфликтует с O_y \& O_x \in T_i \& O_y \in  T_j \& time O_x < time O_y $$ 

Ключевая  теорема - расписание конфликтно сериализуемо тогда и только тогда, когда в графе конфликтов НЕТ циклов. 

s - conflict serializable <=> GC(s) acyclic
Доказательство:
=> 
GC(S) = GC(S*)  так как конфликты только между разными транзакциями.
S* - серийное расписание, а там  Все ребра строго слева на право.

<=
Находим в графе тут вершину, в которую не входит ни одно ребро и ставим все ее операции на первое место, убираем ее и повторяем так далее. Получаем серийное расписание, где все транзакции сгруппированы. = topsort.

#### 2PL (2 phase locking алгоритм )

startT -> Transaction id (TID)
Храним LockTable key->TID, timestamp. таблица mutex 
read/write -> sheduler смотрит в LockTable, если там пусто, то записываем туда TID, если нет, то клиент лочится.
commitT - все lock отпускаются 
abortT - все lock отпускаются 

Когда применять операции клиента? 
Ждем commit. Операции делаем в log. Применяем в KV на commit. commit тоже пихаем в log.
Если sheduler умирает? Так что делаем тот же RSM с Log.

Что если 2 клиента обращаются:
c1 w(x), w(y)
c2 w(y), w(x).
Deadlock. Как решать? 
В памяти mutex были на адресе, там можно сделать что то типа максимальный адрес всегда побеждает. Здесь адреса нет. Будем понимать какая транзакция новее по timestamp.
И тогда сделаем правило при deadlock, что побеждает самая старая транзакция. 

Dead lock - когда цикл в графе конфлитов, тогда abort на том, кто новее. Почему на новом? Гарантируем, что транзакция когда-то выполнится, по этому если бы делали abort не на самой старой этой гарантии быть бы не могло. (ее могли бы постоянно отменять, а так самая старая будет выполняться.)



### Лекция 10. Транзакции продолжение 

#### Повторение Transactions. 2PL.
Api:
start Tx
R/w-> KV storage
commit Tx / abort Tx
Isolation: Для клиента хотим что бы транзакция проходила полностью. T1 T2.
Client->sheduler->K/v Storage.

##### 2Phase-locking алгоритм

LockTable - mutex на каждый ключ 
startTx ->TID
O1(k1), O2(k2) , ... - берем Lock на ключи. LockTable\[k1] = TID.
потом на commit сбрасываем все Lock. time(Lock) < time(Unlock)
Покажем, что расписание полученное таким образом - Serializable. 
Предположим противное и расписания НЕ конфликтно сериализуемы. Значит есть цикл в графе конфликтов.  
Пример цикла T1->T2->...->Tk->T1
Lock(T1)\<**Unlock(T1)** < Lock(T2)\<...\<**lock(T1)** = противоречие, так как всегда time(lock) \<time(unlock)

#### Deadlock фикс
Wound wait.
startTx -> TID, Timestamp
Если поймаем lock, будем смотреть кто старше. Старшая всегда побеждает. 
Если старшая приходит сначала, то abort на младшую. 
Если младшая приходит сначала, то она просто ждет.

Операции пишем в Log. Commit тоже. Операции делаем, когда пришел commit.

Abort. Просто отменяем.

#### Проблемы с performance.
Tx на read-only.
Tx на запись 1го ключа. 
RW mutex. 
На запись mutex будет уникальным. На чтение нет. 

#### Snapshot isolation.
startTx ->timestamp
commitTx -> commitTs.
Отсоединяемся в новый Snapshot и делаем изменения там. 
Если Tx1 intersects Tx2 by write Operation и commit Tx1 < Tx2 то abort Tx2.
Not serializeable.
Пример такой аномалии:
T1:if(read(x) == 0) write(y,1)
T2:if(read(y)== 0) write(x, 1).

В Postgres есть Serializeble IS. Аномалии можно найти в графе конфликтов и исключить.

#### Isolation levels DB.
Проблемы 
1. Dirty reads. 
2. Non-repeatable Reads
3. Phantom reads
Isolation Levels:
1. Serializable
2. Repeatable read
3. Read  committed
4. Read uncommited

#### Возвращение к Snapshot isolation
Read(key, ts)
Write(key, val, ts) <- write(key val)
write = write с commit TS
read = read с start TS 

Поддержим легко через LSM. Ключ будет составной key + ts.
read = lowerbound 
write = insert(keyts)

DB->Tx->KV storage
#### sharded KV storage 
Взяли KV, отсортированное и отшардировали.

Делаем транзакцию внутри 1го шарда.
Меняем Scheduler на Tx manager.
Tx manager на каждый шард.        -> RSM 
LockTable в RAM                                   -> на **лидере** RAFT. Если лидер умер, Transaction abort.
Log на диске                                            ->   RAFT 


Abort:
1. Client
2. Deadlock = wound_wait
3. Leader change = shard update
4. Client die 


Итого, при транзакции в 1м шарде ничего не ломается.

Когда касаемся некольких шардов.
Что если 1 шард abort? Надо и на 2м прокинуть abort. Так как надо поддерживать Atomicity.
Это фиксит алгорит 2PC
#### 2 PC. 2-commit
1. Фаза Prepare(TID). Чекаем состояние шарда. Если он ответил ок, то мы ждем, что когда нибудь он обязан будет ее применить. 
2. Фаза Commit.




### Семинар 10. Шардирование, шард = лидер,DFS. 
Шардирование Key-value DB 
PM - консенсус. N хостов внутри.
key -> value. Value может быть множеством колонок, не важно.

Разделено по таблетам = на какие то части, в отсортировавнном варианте. 
Каждый tablet будет жить на какой то RSM, на N нодах.
PM выбирает ноды для RSM.

Когда node умирает, PM переконфигурирует систему и добавляет другую ноду. 
PM оптимизирует как что делить и балансирует нагрузку.
См. Прошлую лекцию.
#### Оптимизация Raft. 
Так как raft состоит из 
1. Leadeer election 
2. Replication 
То, можно ли оставить на двух остальных чисто диски, а лидера в этой группе оставить, выполняющим инфу. Почему так нельзя? Leader election должен происходить в любом случае, но попробуем его назначать по другому. PM может сказать кто лидер из этих хостов. 
Итого, 1. в raft нам не нужна, leader election будет делать PM.

Было 3 хоста, 1 лидер. на каждом RSM. Compute + storage. 

Теперь 1 лидер у которого compute и независимые 3 хоста, которые являются storage = DFS.

Теперь 2 слоя. 
Compute cloud и DFS. PM берет 1 leader в compute cloud и пишет в DFS. 
#### RAFT over replicated log
Принцип на основе Yt.

Новая сущность Journal. Это набор чанков с некоторыми свойствами, обеспечавиющий консистентную репликацию. = Запись с проверками

Здесь и далее, говорим про 1 таблет.

Journal = совокупность чанков, твечающих за каждый term. Для каждого таблема свои. 
Каждый раз при смене PM'ом лидера term увеличивается, и присваивается новому лидеру.

1. Journal живет на 5 хостах и они не меняются.
	У каждого хоста есть какой то Log. Выбираем максимальный term с кворума хостов, которые ему ответили.

	По сути leader election не помнялся.


Замена replication.
Seal на кворуме хостов. Как и в Raftнадо проверять, у кого там последний Log. Также и здесь будем проверять. 


Jornal - чтение с кворума. 

Когда запись закоммичена? так же как и raft. Лидер в этом терм и предыдущие. 
Есть контрпример. Для него нужно вначале проводить Forbid changes from previous terms on the quorun. Сходить  в хосты, набрать кворум и сказать, что лидер теперь новый. 




В чанк можно делать append. Только с Journal.Можно так как писатель только один - лидер. 
Фиксим количество метаданных через cell, которые являются объединением tablets. То есть RSM становится меньше. 




### Лекция 11. Скипнуто.

### Семинар 11. KV DB: read
Повторение.
#### MVCC. Multi version concurrecncy control.
Ket value timestamp. 
После некоторого момента по timestamp значения устаревают и затираются.

#### Snapshot isolation. 
Способ работы с транзакциями.
Есть глобальное время. У транзакции есть некоторые read timestamp и commit timestamp. 
1. Все чтения внутри транзакции как будто читают данные на момент readtimestamp. То есть получают данные, с timestamp меньше чем readtimestamp.
2. Запись использует committimestamp. 
3. Atomicity. Либо вся транзакция комитится либо ничего из нее
4. Транзакции не могут конфликтовать. Timestamps, writeset? = lockTable? 
#### 2 phase commit 
Есть много шардов и координатор. Координатор хочет закоммитить на сразу несколько шардов. Это все RSM.
Две стадии. 1. Prepare на все шарды. Если есть зотя бы один не ок, то abort транзакции на всех шардах. А если все ответили ок, то 2. Все шарды commit.
#### Как генерировать Timestamp
Обязательно проходят через 1 master. PM или SM.

Наивная реализауия. В RSM поддержим счетчик и при запросе просто будем его увеличивать.

Генерация по batch.
Зачем? Введем proxy timestamp providedrs. К ним приходит запрос generate. Они их собирают некоторое время. получают generated batch и распределают их.
Proxy scalable. По этому колчиество запросов к pm уменьшается 
Время на генерацию запросов увеличилось. => время увеличилось зато rps поднялся.

Идея брать заранее много timestapm почему так нельзя 
нарушаются гарантии транзакций по времени так как ts старый. к примеру будет чтение по старому ts потом. 

#### Чтение без записи в журнал
get Ks -> tablet.
in  tablet SM = memory + LSM.
Sm  get from mem, get from lsm, merge.

В транзакции generate ts не самое первое что есть, у generate commit ts также - он не самый последний. 

если запрос пришел в момент commit но после gen commit ts первой 
то есть есть риск не увидеть commit. 

Как фиксить?
Идея храним в ts last пришедший ts и посчитаем что он закоммитился прям полностью и сохранился.
 Как понимать вот какие из этих ts сохранены?

Идея - делать барьер периодически. = generate ts и запомнить его. хотим что бы все до этого ts было валидно.
В tx есть prepare и commit. Если в tablet нет ни одной транзакции с prepare - то все ок и там валидно. Это такой своеобразный барьер и есть. А если есть prepare то их ждать придется. prepared transactions храним в tablet. 
Если в prepared tx нет пересечения по ключам все тоже ок
Если сеть - lock на ключе то смотрим на prepare ts и определяем по нему 

Схема полагается на монотонные ts от 1 мастера.

Проблема при переезде tablet.(C commit происходит что угодно но это не важно)
Стадия recovery по восстановлению журналов. Тогда надо что бы старый таблет и не читал ничего.

В YT единственное место - полагаемое на  реальное время - если tablet не получает от master ничего первые n секунд, то он больше не читает. Новые tablet тогда первые N секунд ждет. Если master все таки достучался до tablet то задержки этой нет и все сразу работает 

В данном случае даже от физических часов можно получить норм гарантии так как за это время, физические часы дрейфуют на не так много. 

####  
KV DB 
SQL
Transactions
Sharding 
Compute = RSM + RAFT
Storage = DFS + LSM


#### SQL
Query executor 
Направим запрос в tablet 0
Параллелим QE сразу отправляет N запросов 


### Лекция 12. Calvin продолжение.
Tx. 
Start Tx ->TID. (Uniq,Deterministic)
	r/w
commitTx/abortTx.
Мы имеем список детерминированных операций, полученных от client. Запрещаем Random() time.now() и подобное.
Описание на условном SQL.

Имеем некоторый Sequencer(1 машина), который принимает транзакции от client и дает порядок в котором их нужно исполнять дата шардам.Которые последовательно выполняются на шардах.
Транзации батчим, то есть собираем за некоторое время. 

Шардируем Sequencer. Порядок B11, B12,B13, B21... внутри batch по transactionID.
Каждый Sequencer общается с каждым DS, n\*m связей.
Не хотим квадратично расти, введем слой - медиатор.
Sequencer->mediator->DS.
mediator отвечает только за конкретное подмножество шардов.
Каждый seq с каждым med и med с некоторыми ds. получили n\*k + m. 
m>>n;  m>>k.

#### YDB

Sequencer. 
Mediators. Также работа.т через DFS.
Tablets - Одна машина, в которой LSM. А реплиацию на таблеты опускаем в DFS. Позволяет поднять на любой машине в случае ошибки. Не забываем блокировать на write старый лидер в случае их смены.  
DFS 

Сейчас транзакции последовательно, хотим параллельно. 
В 2PL так было можно. 
Конфликтность транзакций.
Есть порядок транзаций от Sequencer. 
Ta и Tb конфликтны если Ta<\_s Tb и ключи и в ta и в b совпадают и 1 из них write.
Distributed TX. 
lock. sequential
execute. parallel
Все lock идут строго до execute

Берем все локи на read и write.
Если лок на получается взять то просто ждем. Транзакции детерминированные, значит оно точно commit.
Избежали deadlock.
Прогресс самой старой транзакции гарантирован.

R(x)->z  W(z, y).  = Обращение по вторичному индексу. Если есть, то разбиваем транзакцию на две. Это еще даже до sequencer. 
Потому что мы должны знать в какие мы ключи ходим заранее.

Чтение хотим раньше записи.
Оптимистичное чтение. Оптимизация.  Еще один locktable = optimisticLocks.
read->opL\[key]->TID.
write->opl\[key]->"". Abort TID.

Можноне обязательноиспользовать TID. TS или что то другое, но не понятно как .

Внутри батча сортим по TID и выполняем в их порядке. 
Теперь ключевое - интерактивные транзакции. Ну просто будем разбивать. 
ReadUncommited write 












## Лекции мфти 
### Лекция 1 

Очень разные типы распределенных системы:
- KV Storage 
- File System
- Coordination service ~atomic
- Message Queues
- Databases

Зачем нужны распределенные системы?
- Горизонтальная масштабируемость 
- Отказоустойчивость
- Проверка корректности данных, если не доверяем каким-то хостам.


Мы ждем от системы гарантий.
Пользователь, node- часть системы.

Message Passing. Внутри системы 
Shared memory. Снаружи системы.

Связь между node обозначим просто проводом. Почему?
Fair-loss link. Не гарантирует доставку. ->
Reliable link. Построим как tcp. по дублированию через id пакетов, по retry до получения Ac.
Вот здесь, мы считаем, что соединение или гарантирует доставку или порвалось. 

Время доставки сообщений:
Асинхронная - нет какого-то конкретного ограничения на время доставки.
- Safety свойство. Не делает плохо.

- Liveness свойство. Иногда делаем хорошее.
Частично-синхронная - существует точка t когда происходит переход safety- liveness.

Partition. Split brain. Система разбивается на две части. Да еще и по разному отвечает и работает.

Сбои node:
- Crash. Умирает и не восстанавливается.
- Restart. Перезагрузка узла 
- Византийские соглашения. Ведет себя хаатично.

Время. 
Надо считать timeout для failure detection а так же делать ordering. 

Для синхронизации часов лучше чем (t2 - t1) / 2 не сделать. То есть сеть работает симметрично. Т к можно доказать что алгоритма синхронизации не существует.

GPS в реальности синхронизирует еще и часы, трех пространственных координат мало. Навигационные  уравнения GPS.  

Google cloud Spanner. Реализуют TrueTime, где TT.now()  возвращает интервал \[e, l\], где настоящее время гарантированно лежит внутри этого интервала. TT.now() работает **ЛОКАЛЬНО**! Устроено это так, что сервера(Time master) синхронизируют время со спутниками GPS. А другие сервера(Armagedon master) содержат в себе атомные часы. И тогда происходит так: в момент времени t\* мы синхронизируется часы, получаем \[e, l\]. Когда через d нам приходит запрос мы возвращаем уже \[e + d \* const, l + d \* const\]. Где const - некоторое количество ppm, заведомо большее чем дрейф локальных часов. Но за счет частой синхронизации, интервал не расширяется на очень много.

### Лекция 2 

Рассмотрим KV Storage 
- Set(key, value)
- Get(key)

Репликация Register. Т.е. как хранить данные, помещающиеся в 1 машину надежно на разных.
Реализация: 
Конкурентная история : (то как в реальности приходили разные запросы к разным узлам системы)\
//////// w1
   ////////// w2
      /////// r
Последовательная история w1 w2 r w3 r. (Как мы пытаемся объяснить себе порядок выполнения)
У регистра есть несколько спецификаций - возможных последовательных историй.  

Модель согласованности - отвечает на вопрос "А какие конкурентные истории может порождать конкретная реализация для заданной спецификации?"

linearizability. ~External consistency
Для любой конкурентной истории существует последовательная история такая ,что, если в Конкурентной истории  одна операция O1 завершилась физически раньше другой O2, то и в последовательной будет сначала O1 потом O2. (Real time ordering)

Если система линеаризуема, то можно думать, что операции типо атомарные и выполняются как-бы **одна за одной**.
Это как раз про то, что операция выполнилась в какой-то определенный момент времени. До нее, операции не было, после она уже выполнена.

Atomic Register.
Single writer. 3 хоста.

Добавим к write timestamp, что бы понимать актуальность данных. Так ка writer 1, то timestamp он просто ++. Write(val, timestamp)
Write, отправляем запрос на все 3 хоста, ждем ответ от 2-х, пишем на 2. После полной записи на 2 хоста всегда отправляем AC клиенту.

Read.  Отправляем всем 3-м. Ждем ответа от 2-х. Получаем max(ts1, ts2).

Обобщаем 3 хоста до 2N + 1 хостов и вводим систему кворумов = N / 2 + 1.

Могут быть асинхронные кворумы, что бы одну операцию делать медленнее, а другую быстрее. Так, что бы увеличить чтение мы можем увеличить кворум на запись, но уменьшить на чтение.

Но если write еще пишет, а нам пришел read, то мы не знаем, про состояние write. Что бы следующий read прочитал как минимум то же что и мы, то после read соберем кворум на запись того, что прочитали.

В линеаризации выстроим все в порядке timestamp и он будет согласован с реальным временем. 

Multiple writer.
Надо выбирать timestamp распределенно монотонно. 
Фаза write:
Соберем кворум на чтение,  достанет max timestamp, возьмем себе timestamp + 1, соберем кворум на запись с нашим новым timestamp.
(может быть еще запись write в процессе, т е делать +1 может не сработать)
Commit Wait. timestamp можно выбрать не собирая кворум на чтение максимальной записи, а просто подождав некоторое время чего-то. 

### Лекция 3 A-Broadcast
Общая задача - построить распределенный DataBase.
Есть Local Storage. Будем реплицировать(дублировать) данные на другие машины. Сверху на KV уже будет SQL.

Linearizability. 
Если в реальности одна операция выполнилась до начала другой, то и в линейном порядке их порядок будет в том же отношении.

Повторение про Multiple writer, single register.

Добавляем операцию CAS.
Проблема. Нужно хорошо понимать значение ячейки, и это должна понимать каждая реплика, а они не синхронизированы. Для того, что бы эти реплики сходилась к 1му значению мы используем timestamp. 
 
Когда мы просто читаем, пишем, то если нам приходит запрос с более старым timestamp,  его можно просто отбросить и забыть про него. А в CAS так нельзя. Значит нужен другой алгоритм.

 Хотим, что бы тот порядок, в котором команды приходят на реплику и был порядком их применения. Да еще и так, что бы этот порядок **у всех** реплик был одинаковым. Это реализует Atomic Broadcast.

Atomic Broadcast:

A-Broadcast(m). Отправляет всем сообщение m. Гарантирует, что рано или поздно на каждом узле, в том числе на исходном, вызовется обработчик.
A-Deliver(m). - Обработчик.

Свойства, которые мы требуем:
Validity - Если несбойный узел запускает Broadcast, то он вызовет обработчик на самом этом узле
Agreement - если какой то несбойный узел доставил сообщение, то и на остальных узлах это сообщение будет доставлено.
Total order - Общий порядок доставки для каждого узла. 

  Для любых двух узлов префиксы доставленных сообщений должны совпадать.
  --------x-----y
  ------x-----y---
  -------y crash node. Такое нельзя.

Рассмотрим State Machine. Автомат с состояниями, состояния переключаются.
Используя A-Broadcast() будем переключать состояние. Когда приедет на узел,  переключим состояние. 
Atomic broadcast как просто транспорт команд. 

Кворумы внутри реализации AB.

Линеаризуемость. Agreement + Total Order => такого быть не может.
Обработчик 2го сообщения на узле 1 должен быть вызван раньше чем обработчик 1го сообщения. Тогда сообщение должно было быть отправлено до момента его отправки 2м узлом.

Важное замечание - операция которая происходит в автомате, которую мы всем узлам даем должна быть детерминированной. (Хэш таблицы, время, дробные числа)

Еще одна проблема. Клиент может не получить AC так как нода умерла. По timeout мы его поймали. Retry мы сделать не можем, так как клиент не знает состояния системы, применилась ли команда или нет? Тогда может вознинуть повторение действия команды. 
Семантика Exactly ones. 

RSM много. Внутри есть Total order. Но снаружи, на всю сеть RSM этого total order нет.

#### **Реализация** Atomic Broadcast.

^fd9023

1. Сообщения нужно доставить
	Reliable broadcast.
		Гарантирует:
		Все сообщения, которые были отправлены корректными узлами будут получены всеми корректными узлами. и все получат один и тот же набор сообщений.
	Отправляем всем свое сообщение. Когда какой-то узел получает первый раз новое сообщение он его отправляет всем. 
	Все. Это и позволяет соблюсти гарантии.


2. Сохрнить порядок сообщений.
	Consensus.
	На каждом узле нам нужен алгоритм propose. На вход дается input. Узлы все вызывают Propose(input). На выход получают некоторый общий out. Этот out должен быть: 
	Из предложенных значений(Validity),
	 Единый для всех узлов. (Agreement),
	 Алгоритм должен завершаться (Temptation).
	
	Будем делать серию консенсусов. 
	A-Broadcast =R-Broadcast
	A-Deliver(m) = R[] += m (Добавим сообщение во множество сообщений, доставленных RB). Для них обработчики доставки еще не вызваны.
	A[] - множество сообщений, для которых обработчики доставки уже были вызваны.
	Фоновый процесс:
		Раунды. r = 0
		while R[] - A[] != 0 (пока есть сообщения, которые еще не были доставлены )
		r = r + 1
		Bi[] = Propose(r, B[]) Предлагаем на консенсус те сообщения, что были нами получены но еще не отправлены R[] - A[].
		На всех сообщениях из Bi[] вызываем обработчик доставки 
		A[] += Bi

	Все. В каждом раунде доставляем какое-то количество сообщений. Все зависит от Propose.

Как решить consensus с помощью AB?
Пусть каждый узел в propose выполнит AB и выберет то, что вернет ему первый обработчик доставки. 
Но мы хотим наоборот AB через consensus и reliable broadcast.


Итого 
RSM > AB > Consensus.
RSM - хорошо, сможем мысли о системе в которой несбойные надежные узлы , но нужно научиться делать Propose.


### Лекция 4 FLP
Невозможность консенсуса, FLP.

По факту AB ~ Consensus. Consensus -> AB (Propose - каждый узел посылает A-Broadcast и выбирает первое значние, которое ему было доставлено.)

Лемма 2. Из бивалентной конфигурации можно перейти в бивалентную конфигурацию. 
Пусть есть C - некоторая бивалентая конфигурация.(При этом ни один узел не был потерян) Есть некоторое множество S - то множество, куда мы можем прийти не отправляя сообщение (m, p).
Но рано или поздно мы должны отправить это (m, p). Подрисуем снизу это множество.
Утверждение. В этом нижнем множестве Se тоже есть бивалентное состояние. Доказательство от противного. 
1. Если в Se есть только унивалентные конфигурации, то там есть хотя бы одна 0 валентная и хотя бы одна 1 валентная. Так как C по условие бивалентная, а через Se мы обязательно пройдем, в Se есть и 0 валентная и 1 валентная.
2.  Утверждение. В S по переходу (m,p) мы попадем в 1 валентность, а по переходу (m', p') и потом (m, p) попадем в 0 валентность. Почему? Пусть цвет исходной вершины С (пусть красный) определиться переходом (m, p) - то, в какое состояние мы попадем если сразу применим переход (пусть 1 валентное). Но! Есть же  состояние другой валентности (зеленое 0 валентности). И есть какой то переход из S в Se зеленое (по (m,p)). Тогда получаем это ребро то, в котором меняется цвет. Получаем, если соединить C и то, откуда мы перешли в зеленое, они будут соседними.  То есть от порядка зависит то, в какие валентности мы попадем.
3. 1.  p' != p Такое невозможно. Почему? По (m,p) попадаем в 1 валентное. Если потом проделаем (m', p') Попадем в 0 валентное, а такое невозможно. 2. p' = p. Состояния C', C'' отличаются только действием p. Убьем узел p. Так как мы должны уметь переживать хотя бы 1 отказ, то найдется какой-то другой путь s который приведет в определенную конфигурацию (пусть 1 валентную). Но! Мы в праве этот путь прикрепить после сообщения (m,p) и (m', p) так как в нем не задействован этот путь s. (m, p) и (m', p) ведут в РАЗНЫЕ валентности. S ведет в каккую-то конкретную. Тогда будет или переход из 0 в 1 валентную или из 1 в 0 валентную по s. Противоречие.
Тогда лемма 1 +  бесконечное число раз лемма 2(сообщения используем в очереди самое старое из недоставленных) = отсутствие Termination. 
Получаем LiveLock.




### Лекция 7 RAFT
Raft. Алгоритм консенсуса. 

Задача репликации log. 
На 1-й node есть RSM, consensus module, log. 
Выбирается лидер. Без лидера алгоритм не работает.

Декомпозируется на 2 типа: выбор лидера и репликация команд при стабильном лидере.

Есть 3 роли: Лидер, Follower, кандидат. Роли взаимоисключающие. 

Время распределено на terms. В terms есть максимум 1 выборы лидера в начале terms и нормальная работа если выборы были успешными.

У каждой реплики своя вот картина terms. Она могла быть выключена и не знать о том, что происходило в системе. 

Heartbeats. 
Followers  поддерживают таймер electionTimeout и обновляют его, если от лидера пришел heartbeat. Если же не получили такого за timeout, то followers считают что лидер умер и начинают выборы нового лидера.

#### Выбор лидера. 
Реплика у которой истек таймер повышает свой term на 1, меняет статус на кандидата и отправляет на остальные ноды запрос на голосование. 
Реплика голосует за первого кандидата в term и запоминает его. 
Тогда будет 3 варианта: или проголосовали за этого кандидата, или выбрали другого(пришло  сообщение, которое только лидер посылает с нашим term) или истек timeout(снова term+1 и повторяем).

**Safety**: В таком сценарии не может быть больше 1го лидера в term. Почему? Каждая реплика голосует не более 1го раза за term. 
**Liveness**: Electiontimeout будем выбирать немного рандомно. (Все ноды изначально followers. Поэтому они  могут просыпаться одновременно и  начать голосовать все только за себя.)  Тогда случится момент, когда 1 сервер проснется и проведер выборы раньше других и все проголосуют за него. 

**Настоящий способ голосования за лидера**
1. В том term мы еще не голосовали
2. Последний term в log у кандидата БОЛЬШЕ или равен, но размер лога у него больше чем у нас 

#### Репликация log.

Log - последовательность слотов с командами и номер term в котором ее положили. Log храним на диске, чтобы переживать рестарты. 
Все операции с log меняют его suffix. Лидер log только добавляет и реплицирует.

Инвариант. 
Если на двух репликах в слоте по одному и тому же индексу лежит запись с одним и тем же term. То это означает что там совпадает эта команда и все префиксы до этой команды.

Лидер при отправке appendentries будет отсылать инфу о предыдущем состоянии и term. Если они не совпадают, то видимо, нужно чинить log у follower'а.

Если у follower там пусто, то будем искать последнее место где была запись, это нам скажет эта реплика. Реплика просто отстает.
Если там есть запись, надо искать такой префикс, что бы реплики совпадали(будем откатываться по степеням 2), причем будем стирать log у follower.

Так как лидер стирает логи других, лидера будем выбирать чуть по другому, аккуратно.

#### Когда команда надежно зафиксирована(закоммичена)?
1. Если ли лидер в терме k положил команду на кворум, то она закоммичена.
2. Если команда придавлена сверху командой из пункта 1.

Из этого следует State machine safety. Если применили команду из закоммиченного лога, то эта команда не поменяется.  
Safety обеспечилось.

 Если не все ноды могут общаться со всеми, то могут быть проблемы. Так, heartbeat некоторые не будут доходить до работающих узлов и они будут вызывать election. Поэтому стабильное состояние не будет достигнуто.
 Решение, часть 1 фаза PreVote(проверка, способна ли node стать лидером).
Виртуальный requestvote. Реплика отвечает положительно, если ее устраивает состояние лога и если она не получает heartbeat от лидера.

Часть2. CheckQuorum.
Проверяет на  лидере, что он вообще собирает кворум.




### Лекция 9. Шардирование
Google colossus. DFS.
#### Шардирование KV DB, таблет - RSM.
Есть большая таблица, не помещающаяся в 1 машину, как хранить?
Поделим ее на части, которые мы уже можем нормально хранить на машинах. 
Каждая такая часть - **таблет**.
Таблеты хранятся на нескольких машинах, которые между собой решают задачу консенуса с помощью к примеру Raft. Эти машины - RSM.
Таблеты не могут быть фиксированного размера, так как ключи меняются, запросы меняются. Больше чем размер диска машины, логично нельзя, но нужно ли меньше? Да, потому что всегда в такой конструкции есть лидер. Лидер может стать узким местом, потому что к нему приходит слишком много запросов. Тогда мы хотим разделить таблет и перебалансировать. А тогда все придется перемещать. Перемещать терабайты в системе не удобно, тогда логично делать **таблеты маленькими - мегабайты.**
**На машинах несколько таблетов.**

Балансировать таблеты надо, кто это делает?
ShardManager, реплицированный.

Храним отдельную таблицу метаданных, отображение таблет - машина.
Вложенность, потом таблица метаданных - метаданные.
В итоге получаем 1 таблет, который вмещается в 1 машину, реплицированно. 

Аналогия с виртуальной памятью в os.

#### Шардирование DFS














### Лекция 11 Транзакции.
2 PL. -про изоляцию многих транзакций.
Пессимистичный. Запрещаем одновременное чтение и запись по одним ключам. 
Start tx - Берем lock;
Commit/abort tx - unlock всех.
Как фиксить deadlock - wound wait. Вводим в api запроса timestamp, 
Если T1 хочет взять лок, но T2 уже владеет этим локом, то если ts1 <(старше) ts2, то ts2 abort. Иначе ждем. 
Итак циклов в графе конфликтов не будет. 


Альтернатива - MVCC + Snapshot isolation. Минусы - появилось WriteSkew. 
У startTx есть readTS, относительно нее - чтения. commitTx - commitTS.
FirstCommiter wins, если есть пересечения по ключам на запись.
Если так вот приходится откатываться, то транзакцию рестартим.

### Лекция 12 Транзакции.

Google Spanner.
Heartbeats от client к transaction manager. 
lock table и transaction manager только на лидере.
Если транзакция только на 1м шарде, то с 2PL все ок. 
Но что если транзакции кросс шардовые?

2PC. - про  отказоустойчивость 1й транзакции.

Coordinator посылает шардам Prepare. Если шард готов рано или поздно принять транзакцию, он ее запоминает и отвечает ок. Если хоть какаой то 1 шард ответил не ок, то транзакция abort.

Если отказ координатора? 
Первый координатор - клиент. Но потом он назначает нового координатора из списка шардов, и потом ответы уже шарду идут.

Оптимизация для read only транзакций, так как там SnapshotIsolation не требуется. 

RW - 2PL + 2PC.
RO - SI.

RO - читает из хранилища на прямую.

T1, T2 пересекаются по ключам, берут локи. Очевидно, что отрезки взятия всех локов и отпускания локов не пересекаются. Значит как то они упорядочились = получили сериализацию, кто первая кто вторая.  
	Commit = запись в хранилище по временной меткой, но как получить строго монотонные упорядоченные временные метки для этих двух транзакций?  

Timestamp Oracle - приходишь к нему, он дает временные метки. 
Простой вариант - RSM с просто fetch add на atomic.
Оптимизация - батчим запросы за n секунд. Делаем 1 вычисление и отвечаем всем клиентам потом. 

У Google - через True Time.





### Семинар 1
UTC - добавляем високосные секунды, что бы время совпадало с наблюдаемыми физическими явлениями.
GPS - в начале добавили немного високосных секунд, что бы синхронизовать с UTC с 1980х.
TAI - атомные, високосных секунд нет, меряем только по физическим процессам в атоме цезия.

Wall time  - Общая точка отсчета для всех node, но ведут себя не монотонно. Это как раз то, что не возможно синхронизировать
Monotonic clock. Разная точка отсчета для всех node, но ведут себя монотонно.
В c++ system_clock - wall clock
steady_clock - monotonic_clock

Leap smear - исправление високосной секунды. 
То есть секунды в компьютере будут идти чуть чуть медленне, что бы убрать эту разницу. 

Single point of failure. Единая точка отказа. Делаем систему так, что бы вот этой точки отказа не было. Что бы потеря какого-то одного оборудования не обрывала бы работу всей системы.

Много маленьких кластеров - не выгодно. Делаем один гигfнnский кластер, что бы было много связей.

Даже у TCP есть проблемы. У client мы не видим полной картины. Сервер может перезагрузиться, выключиться, а tcp подключение на client'е может остаться. Это проблема, это не чиниться, это надо учитывать.
Мб [тут](https://gitlab.com/users/Lipovsky/projects) можно поискать задачи/реализацию.


### Семинар 3. Local Storage, LSM.
- Put(k, v)
- Get(k)
- Delete(k)
- ScanRange(low, high)

Гарантии к бд ACID.
Atomicity. Если запись была начата, а в этот момент машина перезагрузилась, то запись или полностью записалась или полностью не записалась.
Durability. Если запись была подтверждена, а потом машина перезагрузилась, то данные сохранны. 

Hdd. 
Блины, на них дорожки, на дорожке сектора. Сектора по 512Kb. Есть считывающая головка. 
Что бы перейти к другой записи нужно передвинуть иглу на дорожку и доехать до нужного места. 6ms на оборот, в среднем 3ms на доехать до места. А еще seek. В итоге 5-10 ms на поиск. В итоге последовательно все работает гораздо быстрее. 

Ssd.
FlashMemory.
Ячейка->страница->Блок. FTL.
- Read(page) ~10ns
- Programm(page) Переводит все 1 в 0.
- Erase(block) Поставить все в 1.
То есть перезаписывать мы не умеем. Нам выгодно писать в новые страницы. Сборщик мусора помечает старые не валидные страницы в фоне. Но если мы не пишем последовательно, то потом придется двигать данные, что бы освободить место. То есть, из за того, что мы умеем чистить весь блок целиком, валидные данные из блока придется перемещать в другой блок. А вообще, число перезаписей блока ограничено. Поэтому писать последовательно все равно выгодно и нужно. 


Map = Красно-черное дерево, но работает, но требует слишком много доступов в память. Будем использовать B дерево - не 2, а больше ключей в node.  Глубина меньше, значит доступов в память меньше.
B+ дерево.



LSM. Log-structured merge tree.
1. Делаем Get(k).
	Sorted string table (SSTable). Храним отсортированный файл. Делим его на блоки с одинаковым размером offset, у каждого будет некоторый key. Создадим еще таблицу Index с  key->offset. При get найдем по этой новой табличке offset и перейдем в ту часть файла.
2. Быстрый Put(k, v).
	Поддерживаем файл на дискеLog. когда приходит put просто добавляем в конец put(k, v).
3. Быстрый Get(k).
	MemTable в памяти = часть Log. Put = Put_log + Put_memtable. Когда MemTable переполняется кидаем его на диск в SSTable. Итак, в памяти несколько сжатых SSTable с последовательными ссылками друг на друга. Причем, эти SSTable мы будем мерджить в размеры степени двойки, что бы каждый следующий был в два раза больше предыдущего.
4. Delete(k) = Put(k, flag). Когда в фоне будем сливать два SSTable если есть более новое значение с flag то просто будем его и сохранять, запоминая, что этого значения как бы нет.
Оптимизации:
1. BlockCache. Популярные блоки будем хранить там. 
2. BloomFilter.Добавляем его в каждый SSTable. Множество с операциями Add(x) и Contains(x). Причем  Contains отвечает No\Maybe. Берем битовый вектор и k хэш функций. 
	- Когда вставляем ключ вычисляем хэш функции и в те биты ставим 1.
	- Когда ищем x считаем k хэш функций и смотрим на биты, если хотя бы где то нет 1, то ответ NO.
3. MemTable. Реализован через Skiplist.


### Семинар 4. DFS, GoogleFS.

Можем не думать про отказы дисков. Получаем такой PersistentDisk, который выживает при отказе машины. 

API:
- Create(p)
- Delete(p)
- Copy()
- PRead(p, offset, size) - запись в файл по некоторому оффсету. 
- Write(p, offset, data)
- Append(p, data) - offset выбирает сама система. 
Колоссальное отличие в системе будет в зависимости от того будет ли мы использовать write + append или только append.

Обойдемся append.


### Cеминар 9. 2020. Calvin.

В 2PL точно поряда нет, он просто какой то будет. 
Хотим разделить порядок и исполнение. 
Пусть будет Sequencer - который создает порядок транзакций.

Проблема с доступом по вторичному индексу. Решение - оптимистичное чтение .
y = read(x)
write(y).
Оптимистично прочитали y без локов, получили что то, к примеру test
Транзакция становится 
y =read(x)
if(y == test)
write(y)
else 
retry

